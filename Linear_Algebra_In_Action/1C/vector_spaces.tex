\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsfonts,amsthm,amssymb,epigraph,etoolbox,mathtools,setspace,enumitem}  
\usepackage{tikz}
\usetikzlibrary{datavisualization} 
\usepackage[makeroom]{cancel} 
\usepackage[linguistics]{forest}
\usetikzlibrary{patterns}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
\newcommand{\Lim}[1]{\mathrm{lim}(#1)}
\newcommand{\Abs}[1]{\left\vert #1 \right\vert}
\newcommand{\Dom}[1]{\mathrm{dom}\left(#1\right)}
\newcommand{\Range}[1]{\mathrm{range}(#1)}

\newlist{legal}{enumerate}{10}
\setlist[legal]{label=(\alph*)}
\setenumerate[legal]{label=(\alph*)}

\DeclarePairedDelimiter\bra{\langle}{\rvert}
\DeclarePairedDelimiter\ket{\lvert}{\rangle}
\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1\delimsize\vert #2}


\newenvironment{theorem}[2][Theorem]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{result}[2][Result]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{solution}[1][Solution]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\setlength\epigraphwidth{8cm}
\setlength\epigraphrule{0pt}

\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother

\begin{document}
  
 \title{Chapter 1: Vector Spaces}
   \author{Juan Patricio Carrizales Torres}
     \date{Aug 17, 2022}
       \maketitle


       \begin{problem}{1.1}
	Let $\mathcal{V}$ be a vector space over $\mathbb{F}$. Show that if $\alpha,\beta \in \mathbb{F}$ and if $\mathbf{v}$ is a nonzero vector in $\mathcal{V}$, then $\alpha \mathbf{v} = \beta \mathbf{v} \implies \alpha = \beta$. [HINT: $\alpha - \beta \neq 0 \implies \mathbf{v} = (\alpha - \beta)^{-1}(\alpha - \beta)\mathbf{v}$.]
    \begin{proof}
      Suppose, to the contrary, that there are distinct $\alpha,\beta\in \mathbb{F}$ such that for some nonzero $\mathbf{v} \in \mathcal{V}$ we have $\alpha \mathbf{v} = \beta \mathbf{v}$. Then, $\alpha - \beta \neq 0$ and so $\mathbf{v} = (\alpha - \beta)^{-1}(\alpha - \beta)\mathbf{v}$. Hence,  
    \begin{equation*}
      \mathbf{v} = (\alpha-\beta)^{-1} \alpha \mathbf{v} - (\alpha-\beta)^{-1} \beta \mathbf{v}  = (\alpha-\beta)^{-1} (\alpha \mathbf{v} - \beta \mathbf{v}).
    \end{equation*}
    Since $\alpha \mathbf{v} = \beta \mathbf{v}$, it follows that $\alpha \mathbf{v} - \beta \mathbf{v} = \beta \mathbf{v} - \beta \mathbf{v} = \mathbf{0}$. This implies that $\mathbf{v} = (\alpha - \beta)^{-1} \mathbf{0} = \mathbf{0}$. This is a contradiction to our assumption that $\mathbf{v}$ was nonzero.\\

    Another way to prove this directly is by using the fact, for some $\alpha \in \mathbb{F}$ and nonzero vector $\mathbf{v}$, that $\alpha\mathbf{v} = \mathbf{0} \implies \alpha = 0$. A proof reads as follows:\\

    Let $\alpha,\beta \in \mathbb{F}$ and $\mathbf{v}\in \mathcal{V}$ be some nonzero vector such that $\alpha \mathbf{v} = \beta \mathbf{v}$. Then, $\alpha \mathbf{v} - \beta \mathbf{v} = \beta \mathbf{v} - \beta \mathbf{v} = \mathbf{0}$ and so $(\alpha - \beta) \mathbf{v} = \mathbf{0}$. Since $\mathbf{v}$ is nonzero, it follows that $\alpha - \beta = 0$ and so $\alpha = \beta$.
    \end{proof}
    \end{problem}
    \begin{problem}{1.2}
      Show that the space $\R^{3}$ endowed with the rule
    \begin{equation*}
      \mathbf{x}\; \square \; \mathbf{y} = \begin{bmatrix} \max(x_{1},y_{1}) \\ \max(x_{2},y_{2}) \\ \max(x_{3},y_{3}) \end{bmatrix}
    \end{equation*}
    for vector addition and the usual rule for scalar multiplication is not a vector space over $\R$. 
    \begin{proof}
      We show that this space has no unique additive identity. Consider some $\mathbf{x} = (x_{1},x_{2},x_{3})$. Then, both $\mathbf{y} = (x_{1}-1,x_{2}-1,x_{3}-1)$ and $\mathbf{z} = (x_{1}-2, x_{2}-2, x_{3}-2)$ are in $\R^{3}$ and they are distinct. Note that $\mathbf{x} \; \square \; \mathbf{y} = \mathbf{x}$ and $\mathbf{x} \; \square \; \mathbf{z} = \mathbf{x}$. \\

      In fact, one can easily show that there is no vector that is an additive inverse of every vector (the zero vector $\mathbf{0}$) since one can easily construct a vector with elements lower than the ones from any other vector.
    \end{proof}
    \end{problem}
    \begin{problem}{1.3}
      Let $\mathcal{C}\subset \R^{3}$ denote the set of vectors $\mathbf{a} = \begin{bmatrix} a_{1}\\ a_{2}\\ a_{3}\end{bmatrix}$ such that th polynomial $a_{1} + a_{2}t + a_{3}t^{2} \geq 0$ for every $t\in \R$. Show that it is closed under vector addition (i.e., $\mathbf{a},\mathbf{b} \in \mathcal{C} \implies \mathbf{a}+\mathbf{b} \in \mathcal{C}$), but that $\mathcal{C}$ is not a vector space over $\R$. [REMARK: A set $\mathcal{C}$ with the indicated two properties is called a $\mathbf{cone}$.]
    \begin{proof}
      We first show that $\mathcal{C}$ is closed under addition. Consider any $\mathbf{a}, \mathbf{b}\in \mathcal{C}$. Then, for every $t\in \R$ we have $a_{1} + a_{2}t + a_{3}t^{2} \geq 0$ and $b_{1}+b_{2}t+b_{3}t^{2}\geq 0$. Then, 
    \begin{align*}
      a_{1} + a_{2}t + a_{3}t^{2} + b_{1}+b_{2}t+b_{3}t^{2} &= (a_{1} + b_{1}) + (a_{2} + b_{2})t + (a_{3}+b_{3})t^{2} \geq 0
    \end{align*}
    for every $t\in \R$. Thus, $\mathbf{a} + \mathbf{b} = \begin{bmatrix} a_{1}+b_{1}\\ a_{2} + b_{2}\\ a_{3}+b_{3} \end{bmatrix} \in \mathcal{C}$. However, it is not close under scalar multiplication. Consider some nonzero $\mathbf{v} \in \mathcal{C}$ and let $\alpha = -1$. Since $v_{1} + v_{2}t + v_{3}t^{2} \geq 0$ for every $t\in \R$, it follows that $(-v_{1}) + (-v_{2})t + (-v_{3})t^{2}<0$ for every positive $t$. Hence, $(-1)\mathbf{v} \not\in \mathcal{C}$ and so it is not a vector space over $\R$.
    \end{proof}
    \end{problem}
    \begin{problem}{1.5}
      Let $\mathcal{F}$ denote the set of continuous real-valued functions $f(x)$ on the interval $0\leq x\leq 1$. Show that $\mathcal{F}$ is a vector space over $\R$ with respect to the natural rules of vector addition $\left( \left( f_{1} + f_{2} \right)(x) = f_{1}(x) + f_{2}(x) \right)$ and scalar multiplication $\left( (\alpha f)(x) = \alpha f(x) \right)$.
      \begin{proof}
      \begin{enumerate}
	\item \textbf{Closed under vector addition}

	  Consider two functions $f,g \in \mathcal{F}$. Let $x\in [0,1]$. Then, $f(x), g(x) \in \R$ and so $(f + g)(x) = f(x) + g(x) \in \R$ since $\R$ is closed under addition. Therefore, $f+g$ is a real-valued function on the interval $[0,1]$ and so $(f+g)\in \mathcal{F}$.
	\item \textbf{Closed under scalar multiplication}

	  Consider some function $f\in \mathcal{F}$ and real number $\alpha$. Let $x\in [0,1]$. Then, $f(x)\in \R$ and so $ (\alpha f)(x) = \alpha f(x) \in \R$ since $\R$ is closed under multiplication. Thus, $\alpha f$ is a real-valued function on the interval $[0,1]$ and so $\alpha f \in \mathcal{F}$.  
	\item \textbf{Vector addition is commutative}

	  Let $f,g\in \mathcal{F}$ and $x\in [0,1]$. Then, $(f+g)(x) = f(x) + g(x) = g(x) + f(x) = (g+f)(x)$ since addition in the set of real numbers is commutative.
	\item \textbf{Vector addition is associative}

	  Let $f,g,h \in \mathcal{F}$ and $x\in [0,1]$. Then, $\left( \left( f+g \right)+h \right)(x) = (f+g)(x) + h(x) = f(x)+g(x)+h(x) = f(x) + (g+h)(x) = \left( f +\left( g+h\right) \right)(x)$ since addition in $\R$ is associative (the order of addition does not matter).
	\item \textbf{Existence of additive identity}

	  Let $f:[0,1]\to \R$ be defined by $f(x) = 0$ for all $x\in [0,1]$. Then, $f$ is a continous real-valued function and so $f\in \mathcal{F}$. Consider any $g\in \mathcal{F}$ and let $a\in [0,1]$. Then, $(f + g)(a) = f(a) + g(a) = 0+ g(a) = g(a)$ since $0$ is the additive identity  of real numbers. Thus, $f$ is an additive identitive in $\mathcal{F}$. 
	\item \textbf{Existence of additive inverse}

	  Consider some $f\in \mathcal{F}$. Let $g:[0,1]\to \R$ be defined by $g(x) = -f(x)$ for all $x\in [0,1]$. Consider some $x\in [0,1]$ and so $(f+g)(x) = f(x) + g(x) = f(x) -f(x) = 0$. Hence, $g$ is the additive inverse of $f$.
	\item $f \in \mathcal{F} \implies (1)f = f$
	  
	  Let $f\in \mathcal{F}$. Consider any $x\in[0,1]$ and so $f(x) = (1)f(x)$. Thus, $f = (1)f$. 
      \item For any $\alpha,\beta \in \R$ and vector $f \in \mathcal{F}$, $\alpha(\beta f) = (\alpha\beta) f$

       Let $f\in \mathcal{F}$ and $\alpha,\beta \in \R$. Consider any $x\in[0,1]$ and so $\alpha(\beta f)(x) = \alpha(\beta f(x)) = (\alpha\beta)f(x)$ since multiplication in $\R$ is associative. Thus, $\alpha(\beta f) = (\alpha \beta)f$
      \item For any $\alpha,\beta \in \R$ and vector $f \in \mathcal{F}$, $(\alpha + \beta) f = \alpha f + \beta f$

	Let $f\in \mathcal{F}$ and $\alpha,\beta \in \R$. Consider any $x\in [0,1]$ and so $(\alpha + \beta)f(x) = \alpha f(x) + \beta f(x) = (\alpha f + \beta f)(x)$ since multiplication over addition is distributive for real numbers.
    \end{enumerate}
    \end{proof}
    \end{problem}
    \begin{lemma}{1}
      Let $\mathcal{S}$ be a nonempty subset of a vector space $\mathcal{M}$ over $\mathbb{F}$. Then, $\mathcal{S}$ is a vector space if and only if for every pair of vectors $\mathbf{v}, \mathbf{a} \in \mathcal{S}$ and $\alpha, \beta\in \mathbb{F}$, $\alpha \mathbf{v} + \beta \mathbf{a} \in \mathcal{S}$.
    \end{lemma}
    \begin{proof}
	Assume that $\mathcal{S}$ is a vector space and so it is closed under addition and scalar multiplication. Let $\mathbf{v},\mathbf{a} \in \mathcal{S}$ and $\alpha, \beta \in \mathbb{F}$, then $\alpha \mathbf{v}, \beta \mathbf{a} \in \mathcal{S}$ and so $\alpha \mathbf{v} + \beta \mathbf{a} \in \mathcal{S}$.\\
	Suppose, for every pair of vectors $\mathbf{v}, \mathbf{a} \in \mathcal{S}$ and $\alpha, \beta\in \mathbb{F}$, that $\alpha \mathbf{v} + \beta \mathbf{a} \in \mathcal{S}$. Let $\alpha = 0$ and $\beta\in \mathbb{F}$. Consider any vectors $\mathbf{v},\mathbf{a} \in \mathcal{S}$. Then, $\alpha \mathbf{v} = 0$ is the additive identity of $\mathcal{M}$ and so $\beta\mathbf{a} = \alpha \mathbf{v} + \beta\mathbf{a} \in \mathcal{S}$. Thus, $\mathcal{S}$ is closed under scalar multiplication.\\
	Consider some vectors $\mathbf{v},\mathbf{a}\in \mathcal{S}$ and let $\alpha = \beta = 1$. Then, $\mathbf{v} + \mathbf{a} = (1)\mathbf{v} + (1)\mathbf{a} = \alpha\mathbf{v} + \beta \mathbf{a} \in \mathcal{S}$ since $\mathbf{v}, \mathbf{a} \in \mathcal{M}$. Therefore, $\mathcal{S}$ is closed under addition and so it is a vector space.
    \end{proof}
    \begin{problem}{1.6}
      Let $F_{0}$ denote the set of continuous real-valued functions $f(x)$ on the interval $0\leq x \leq 1$ that met the auxiliary constraints $f(0) =0$ and $f(1) = 0$. Show that $F_{0}$ is a vector space over $\R$ with respect to the natural rules of vector addition and scalar multiplication that were introducd in \textbf{Excercise 1.5} and that $F_{0}$ is a subspace of the vector space $\mathcal{F}$ that was considered there.
    \end{problem}
    \begin{proof}
      By definition, $F_{0} \subseteq \mathcal{F}$. Let's prove that it is closd under addition and scalar multiplication. Consider some $f, g\in F_{0}$ and $\alpha,\beta\in \R$. Then, $\alpha f + \beta g$ is a real-valued function since $f,g\in \mathcal{F}$. Particularly, $(\alpha f + \beta g)(0) = \alpha f(0) + \beta g(0) = 0 + 0 = \alpha f(1) + \beta g(1) = (\alpha f+ \beta g)(1)$ and so, by condition, it is a vector in $F_{0}$. Therefore, $F_{0}$ is a subspace of $\mathcal{F}$. 
    \end{proof}
    \begin{problem}{1.7}
      Let $F_{1}$ denote the set of continuous real-valued functions $f(x)$ on the interval $0\leq x \leq 1$ that meet the auxiliary constraints $f(0)=0$ and $f(1)=1$. Show that $F_{1}$ is not a vector space over $\R$ with respect to the natural rules of vector addition and scalar multiplication that were introduced in \textbf{Exercise 1.5}.
    \end{problem}
    \begin{proof}
      We know that $F_{1}\subseteq \mathcal{F}$. Consider some $f\in F_{1}$. Then, $(2)f$ is a continuous real-valued function since $f\in \mathcal{F}$. However, note that $(2f)(1) = (2)f(1) = 2 \neq 1$ and so $(2)f\not\in F_{1}$. Hence, $F_{1}$ is not closed under scalar multiplication and so $F_{1}$ is not a subspace of $\mathcal{F}$. 
    \end{proof}
    \begin{problem}{1.8}
      Verify the last assertion; i.e., if $\left\{ \mathbf{v}_{1}, \mathbf{v}_{2}, \mathbf{v}_{3}, \dots, \mathbf{v}_{k} \right\}$ is a set of linearly independent vectors in the space $\mathcal{V}$ over $\mathbb{F}$ and if $\mathbf{v} = \alpha_{1}\mathbf{v}_{1} + \alpha_{2}\mathbf{v}_{2} + \dots+\alpha_{k}\mathbf{v}_{k} = \beta_{1}\mathbf{v}_{1} + \beta_{2}\mathbf{v}_{2}+ \dots+ \beta_{k}\mathbf{v}_{k}$, where $\alpha_{j},\beta_{j}\in \mathbb{F}$ for $j=1,\dots,k$, then $\alpha_{j} = \beta_{j}$ for $j=1,\dots,k$.
    \end{problem}
    \begin{proof}
      Let $\left\{ \mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{k} \right\}$ be a set of linearly independet vectors in the space $\mathcal{V}$ over $\mathbb{F}$. Furthermore, assume that there is some vector $\mathbf{v}\in \mathcal{V}$ such that $\mathbf{v} = \alpha_{1}\mathbf{v}_{1} + \alpha_{2}\mathbf{v}_{2} + \dots+\alpha_{k}\mathbf{v}_{k} = \beta_{1}\mathbf{v}_{1} + \beta_{2}\mathbf{v}_{2}+ \dots + \beta_{k}\mathbf{v}_{k}$, where $\alpha_{j},\beta_{j}\in \mathbb{F}$ for $j=1,\dots,k$. Because $\mathbf{v}\in \mathcal{V}$, it follows that
    \begin{align*}
      \mathbf{v} - \mathbf{v} &= \alpha_{1}\mathbf{v}_{1} + \alpha_{2}\mathbf{v}_{2} + \dots+\alpha_{k}\mathbf{v}_{k} - \left(\beta_{1}\mathbf{v}_{1} + \beta_{2}\mathbf{v}_{2}+ \dots + \beta_{k}\mathbf{v}_{k}\right)\\
      &= (\alpha_{1}-\beta_{1})\mathbf{v}_{1} + (\alpha_{2}-\beta_{2})\mathbf{v}_{2} + \dots + (\alpha_{k}-\beta_{k})\mathbf{v}_{k} = 0.
    \end{align*}
    Since $\mathbf{v}_{1}, \mathbf{v}_{2},\dots,\mathbf{v}_{k}$ are linearly independent, it follows that $\alpha_{j} - \beta_{j}=0$ and so $\alpha_{j}=\beta_{j}$ for $j=1,\dots,k$.
    \end{proof}
    \begin{problem}{1.10}
      Show that if
    \begin{align*}
      A &= \begin{bmatrix} a_{11} & a_{12} & a_{13} \\  a_{21} & a_{22} & a_{23} \end{bmatrix} &\text{ and }&  &B &= \begin{bmatrix} b_{11} & b_{12} & b_{13} & b_{14} \\ b_{21} & b_{22} & b_{23} & b_{24} \\ b_{31} & b_{32} & b_{33} & b_{34} \end{bmatrix}
    \end{align*}
    then 
    \begin{align*}
      AB = \begin{bmatrix} a_{11} & 0 & 0\\ a_{21} & 0 & 0 \end{bmatrix} B +
       \begin{bmatrix} 0 & a_{12} & 0 \\ 0 & a_{22} & 0  \end{bmatrix} B +
       \begin{bmatrix} 0 & 0 & a_{13} \\ 0 & 0 & a_{23}\end{bmatrix} B 
    \end{align*}
    and hence that
    \begin{align*} 
      AB = \begin{bmatrix} a_{11}\\ a_{21}\end{bmatrix} \begin{bmatrix} b_{11} & \dots & b_{14} \end{bmatrix}  +
      \begin{bmatrix} a_{12} \\  a_{22}  \end{bmatrix} \begin{bmatrix} b_{21} & b_{22} & b_{23} & b_{24} \end{bmatrix} +
      \begin{bmatrix} a_{13} \\ a_{23}\end{bmatrix} \begin{bmatrix} b_{31} & b_{32} & b_{33} & b_{34} \end{bmatrix}
    \end{align*}
    \end{problem}
    \begin{proof}
      By the definition of addition of matrices
    \begin{align*}
      A = \begin{bmatrix} a_{11} & 0 & 0\\ a_{21} & 0 & 0 \end{bmatrix}  +
       \begin{bmatrix} 0 & a_{12} & 0 \\ 0 & a_{22} & 0  \end{bmatrix}  +
       \begin{bmatrix} 0 & 0 & a_{13} \\ 0 & 0 & a_{23}\end{bmatrix}  
    \end{align*}
    and so
    \begin{align*}
      AB = \begin{bmatrix} a_{11} & 0 & 0\\ a_{21} & 0 & 0 \end{bmatrix} B +
       \begin{bmatrix} 0 & a_{12} & 0 \\ 0 & a_{22} & 0  \end{bmatrix} B +
       \begin{bmatrix} 0 & 0 & a_{13} \\ 0 & 0 & a_{23}\end{bmatrix} B 
    \end{align*}
    since the multiplication of matrices is distributive over addition. By the definition of matrix multiplication, each entry $c_{kl} = \sum_{j=1}^{q} a_{kj}b_{jl}$, for the rows $k  = 1,\dots,p$ and columns $l= 1,\dots,r$. Note that each matrix component of $A$ has just one nonzero column $m$ and so each entry $c_{kl} = a_{km}b_{ml}$. Thus 
    \begin{align*}
      AB &= \begin{bmatrix} a_{11}b_{11} & a_{11}b_{12} & a_{11}b_{13} & a_{11}b_{14}\\ a_{21}b_{11} & a_{21}b_{12} & a_{21}b_{13} & a_{21}b_{14} \end{bmatrix}  + \begin{bmatrix} a_{12}b_{21} & a_{12}b_{22} & a_{12}b_{23} & a_{12}b_{24}\\ a_{22}b_{21} & a_{22}b_{22} & a_{22}b_{23} & a_{22}b_{24} \end{bmatrix}  \\
      &+ \begin{bmatrix} a_{13}b_{31} & a_{13}b_{32} & a_{13}b_{33} & a_{13}b_{34}\\ a_{23}b_{31} & a_{23}b_{32} & a_{23}b_{33} & a_{23}b_{34} \end{bmatrix}\\
     &= \begin{bmatrix} a_{11}\\ a_{21}\end{bmatrix} \begin{bmatrix} b_{11} & \dots & b_{14} \end{bmatrix}  +
      \begin{bmatrix} a_{12} \\  a_{22}  \end{bmatrix} \begin{bmatrix} b_{21} & b_{22} & b_{23} & b_{24} \end{bmatrix} +
      \begin{bmatrix} a_{13} \\ a_{23}\end{bmatrix} \begin{bmatrix} b_{31} & b_{32} & b_{33} & b_{34} \end{bmatrix}
    \end{align*}
    \end{proof}
    \begin{problem}{1.12}
      Show that if $A$ and $B$ are invertible matrices of the same size, then $AB$ is invertible and $(AB)^{-1} = B^{-1}A^{-1}$. 
    \begin{proof}
      Let $A,B\in \mathbb{F}^{p\times p}$ be invertible matrices. Then, $A^{-1}$ and $B^{-1}$ are left-right inverses of $A$ and $B$, respectively. Therefore,
    \begin{align*}
      (AB)(B^{-1}A^{-1}) &= A(BB^{-1})A^{-1}\\
      &= A(I_{p}A^{-1}) = AA^{-1}\\
      &= I_{p}
    \end{align*}
    and
    \begin{align*}
      (B^{-1}A^{-1})(AB) &= B^{-1}(A^{-1}A)B\\
      &= B^{-1}(I_{p}B) = B^{-1}B\\
      &= I_{p},
    \end{align*}
    since matrix multiplication is associative. Thus, $B^{-1}A^{-1}$ is the \textbf{inverse} of $AB$ and so $AB$ is invertible. 
    \end{proof}
    \end{problem}
    \begin{problem}{1.13} Show that the matrix $A=\begin{bmatrix} 1 & 0 & 1\\ 1 & 1 & 0\\ 1 & 1 & 0 \end{bmatrix}$ has no left inverses and no right inverses.
    \begin{proof}
      Suppose to the contrary, that $A$ has some right inverse $B$. Then, $B\in \mathbb{F}^{3\times 3}$ and $AB=C=I_{3}$. Therefore, $c_{22} = 1\cdot b_{12} + 1\cdot b_{22} + 0\cdot b_{32} = 1$ and $c_{32} = 1\cdot b_{12} + 1\cdot b_{22} + 0\cdot b_{32} = 0$, which is a contradiction.\\
      Now, assume, to the contrary, that $A$ has some left inverse $B$. Then, $B\in \mathbb{F}^{3\times 3}$ and $BA = C = I_{3}$. Hence, $c_{11} = b_{11}\cdot 1 + b_{12}\cdot 1 + b_{13}\cdot 1 = 1$, $c_{12} =  b_{11}\cdot 0 + b_{12}\cdot 1 + b_{13}\cdot 1 = 0$ and $c_{13} = b_{11}\cdot 1 + b_{12}\cdot 0+ b_{13}\cdot 0 = 0$. This leads to the contradiction $1=0$.
    \end{proof}
    \end{problem}
    \begin{problem}{1.15}
      Show that if a matrix $A\in \mathbb{C}^{p\times q}$ has two right inverse $B_{1}$ and $B_{2}$, then $\lambda B_{1} + (1-\lambda)B_{2}$ is also a right inverse for every choice of $\lambda \in \mathbb{C}$.
\begin{proof}
  Suppose that $A$ has two right inverses $B_{1},B_{2}\in \mathbb{C}^{q\times p}$. Choose any $\lambda \in \mathbb{C}$. Then
\begin{align*}
  A\left( \lambda B_{1} + (1-\lambda)B_{2} \right) &= \lambda AB_{1} + (1-\lambda)AB_{2}\\
  &= \lambda I_{p} + (1-\lambda)I_{q} = (\lambda - \lambda)I_{p} + I_{p}\\
  &= I_{p}.
\end{align*}
since matrix multiplication is distributive and  under scalar multiplication is commutative. Assuming that another matrix $A'$ has two left inverses $B_{1},B_{2}\in \mathbb{C}^{q\times p}$ and let $\lambda \in \mathbb{C}$. Then,
\begin{align*}
  \left( \lambda B_{1} + (1-\lambda)B_{2} \right)A &= \lambda B_{1}A + (1-\lambda)B_{2}A\\
  &= \lambda I_{q} + (1-\lambda)I_{q} = (\lambda - \lambda)I_{q} + I_{q}\\
  &= I_{q}.
\end{align*}
\end{proof}
\section{INTERESTING LEMMAS}

\begin{lemma}{1}
  Let $A\in \mathbb{F}^{p\times q}$. $A$ is right-invertible if and only if the rows are linearly independent. The same can be said for left-invertibility and columns.
\begin{proof}
  Assume that the rows of $A$ are linearly independent. We show that we can construct a right-inverse $B\in \mathbb{F}^{q\times p}$. 
\end{proof}
\end{lemma}
\begin{problem}{1.16}
  Show that a given matrix $A\in \mathbb{F}^{p\times q}$ has either 0, 1 or infinitely many right inverses and that the same conclusion prevails for left inverses.
\end{problem}
\begin{proof}
  Consider the vector space $\mathbb{F}^{p\times q}$ with $p,q\geq 2$. Consider the zero matrix $\mathbf{0}\in \mathbb{F}^{p\times q}$ and so it has no left and right invertibles since $A\mathbf{0} = \mathbf{0}B = \mathbf{0}$ for all $A,B\in \mathbb{F}^{q\times p}$. \\
  Now, let's construct some matrix $A\in \mathbf{F}^{p\times q}$. Now, let each entry $a_{ii}=1$ while the other be zero. For instance, in the case $p>q$, we have that 
\begin{align*}
  A &= 
\begin{bmatrix}
  1 & 0 & 0 & 0 & \dots & 0 \\
  0 & 1 & 0 & 0 &  \dots & 0\\
  0 & 0 & 1 & 0 &  \dots & 0\\
   &   &  \vdots & \vdots & & \vdots\\
   0 & 0 & 0 & 0 &  \dots & 1\\
   0 & 0 & 0 & 0 &  \dots & 0\\
   0 & 0 & 0 & 0 &  \dots & 0\\
   0 & 0 & 0 & 0 & \dots & 0.
\end{bmatrix}
\end{align*}
If $p\geq q$ (greater or equal number of rows), then $A^{T}A = I_{p}$. On the other hand, if $q\geq p$ (greater or equal numbr of columns), then $AA^{T} = I_{q}$.\\

Hence, any matrix $A\in \mathbb{F}^{p\times q}$ can have 0 or at least one right/left invertible (depending on the order relation of rows and columns). If it has more than one right/left invertibles, then one can construct and infinity of right/left invertibles with the formula given in \textbf{Problem 1.15}.

\end{proof}
    \end{problem}
\end{document}


