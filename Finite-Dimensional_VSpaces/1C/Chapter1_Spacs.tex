\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsfonts,amsthm,amssymb,epigraph,etoolbox,mathtools,setspace,enumitem}  
\usepackage{tikz}
\usetikzlibrary{datavisualization} 
\usepackage[makeroom]{cancel} 
\usepackage[linguistics]{forest}
\usetikzlibrary{patterns}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
\newcommand{\Lim}[1]{\mathrm{lim}(#1)}
\newcommand{\Abs}[1]{\left\vert #1 \right\vert}
\newcommand{\Dom}[1]{\mathrm{dom}\left(#1\right)}
\newcommand{\Range}[1]{\mathrm{range}(#1)}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}

\newlist{legal}{enumerate}{10}
\setlist[legal]{label=(\alph*)}
\setenumerate[legal]{label=(\alph*)}

\DeclarePairedDelimiter\bra{\langle}{\rvert}
\DeclarePairedDelimiter\ket{\lvert}{\rangle}
\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1\delimsize\vert #2}


\newenvironment{theorem}[2][Theorem]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{result}[2][Result]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{solution}[1][Solution]{\begin{trivlist} \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\setlength\epigraphwidth{8cm}
\setlength\epigraphrule{0pt}

\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother

\begin{document}
  
\title{Chapter 1: Spaces}
\author{Juan Patricio Carrizales Torres}
\date{Sep 29, 2022}
\maketitle
\section{Fields}

In Linear Algebra, we will be working with numbers from any type of class/set. Hence, to simplify things and make them more general, we will introduce the idea of fields. A \textbf{field} is a set of objects (including numbers) called \textbf{scalars} with operations of addition and multiplication that fulfill the following rules (let $\alpha$ and $\beta$ be scalars):
\begin{enumerate}
  \item \textbf{Addition}
\begin{enumerate}
  \item commutatitivity, $\alpha+\beta = \beta + \alpha$.
  \item associativity, $\alpha + (\beta + \gamma) = (\alpha + \beta) + \gamma$.
  \item additive identity, there is a unique scalar $0$ such that for every scalar $\alpha$, $\alpha + 0 = \alpha$.
  \item additive inverse, for each scalar $\alpha$ there is a unique scalar $-\alpha$ such that $\alpha + (- \alpha) = 0$.
\end{enumerate}
\item \textbf{Multiplication}
\begin{enumerate}
  \item commutativity, $\alpha\beta = \beta\alpha$.
  \item associativity, $\gamma(\alpha\beta) = (\gamma\alpha)\beta$.
  \item multiplicative identity, there is a unique nonzero scalar $1$ for every scalar $\alpha$ such that $1\alpha = \alpha$.
  \item multiplicative inverse, for every nonzero scalar $\beta$, there is a unique $\beta^{-1}$ such that $\beta\beta^{-1}=1$.
\end{enumerate}
\item \textbf{Linearity}
\begin{enumerate}
  \item Multiplication is distributive over addition, $\alpha(\beta + \gamma) = \alpha\beta + \alpha\gamma$.
\end{enumerate}
\end{enumerate}
For instance, the class of real numbers and the class of complex numbers are fields.
\subsection{Excercises}
\begin{problem}{1}
  Almost all the laws of elementary arithmetic are consequences of the axioms defining a field. Prove, in particular, that if $\mathcal{F}$ is a field, and if $\alpha,\beta$ and $\gamma$ belong to $\mathcal{F}$, then the following relations hold.
\begin{enumerate}
  \item $0+\alpha = \alpha$
\begin{proof}
  Due to the commutativity property of addition, $\alpha = \alpha + 0 = 0 + \alpha$.
\end{proof}
  \item If $\alpha + \beta = \alpha + \gamma$, then $\beta = \gamma$.
\begin{proof}
  Due to the additive inverse, associativity and commutativity, $\alpha + \beta + (-\alpha) = \alpha + (\beta + (-\alpha)) = ( \alpha + (-\alpha))+\beta = \beta = \gamma$.
\end{proof}
  \item $\alpha + (\beta-\alpha) = \beta$.
\begin{proof}
  Just like in $(b)$, 
\begin{align*}
  \alpha + (\beta+(-\alpha)) &= \alpha + (-\alpha + \beta)\\
  &= (\alpha + (-\alpha)) +\beta = 0 + \beta\\
  &= \beta.
\end{align*}
\end{proof}
\item $\alpha\cdot 0 = 0\cdot \alpha = 0$. (In this case, the dot indicates multiplication).
\begin{proof}
  Note that
\begin{align*}
  0\cdot \alpha + (-0\cdot\alpha) &= 0 = (0+0)\alpha + (-0\cdot\alpha)\\
  &= 0\cdot\alpha + (0\cdot\alpha + (-0\cdot\alpha)) = 0\cdot\alpha\\
  &= \alpha\cdot 0
\end{align*}
\end{proof}
\item $(-1)\alpha = -\alpha$
\begin{proof}
  Observe that
\begin{align*}
  \alpha + (-\alpha) &= 0 = 0\alpha\\
  &= (1-1)\alpha = \alpha + (-1)\alpha.
\end{align*}
By $(b)$, $-\alpha = (-1)\alpha$.
\end{proof}
\item $(-\alpha)(-\beta) = \alpha\beta$.
\begin{proof}
  By $(e)$, $(-\alpha)(-\beta) = ((-1)\alpha)((-1)\beta)$. Then,
\begin{align*}
  ( (-1)\alpha)( (-1)\beta) &= (-1)(\alpha( (-1)\beta))\\
  &= (-1)( (\alpha(-1))\beta) = (-1)( ((-1)\alpha)\beta)\\
  &= ( (-1)( (-1)\alpha))\beta = ( ( (-1)(-1))\alpha)\beta\\
&= (1\alpha)\beta = \alpha\beta
\end{align*}
\end{proof}
\item $\alpha\beta = 0 \implies \alpha = 0 \text{ or }\beta=0$.
\begin{proof}
  Let $\alpha\beta = 0$. Note that either $\alpha = 0$ or $\alpha \neq 0$. In the first case, the result is true. In the case of the latter, there is some $\alpha^{-1}$ and so $\alpha^{-1}\alpha\beta = 1\beta = \alpha^{-1}0 = 0$. 
\end{proof}
\end{enumerate}
\end{problem}
\begin{problem}{2}
\begin{enumerate}
  \item Is the set of all positive integers a field? (In familiar systems, such as the integers, we shall almost always use the ordinary operations of addition and multiplication. On the rare occasions when we depart from this convention, we shall give ample warning. As for ``positive'', by that word we mean, here and elsewhere in this book, ``greater than or equal to zero''. If $0$ is to be excluded, we shall say ``strictly positive''.)
\begin{solution}
  It is not a field. Although the commutativity, associativity and linearity of closed addition and multiplication is maintained, there is and additive identitiy 0 and the multiplicative identity 1 is present in this set, there are no additive inverses and multiplicative inverses.
\end{solution}
\item What about the set of integers?
\begin{solution}
  It is still not a field. It just needs some type of identity multiplicative.
\end{solution}
\item Can the answers to these questions be changed by re-defining addition or multiplication (or both)?
\begin{solution}
  We can re-define addition and multiplication so that there are multiplicative identities for every positive integers. Consider some integer $\alpha$. Let's mantain all known properties but make this small change
\begin{align*}
  \alpha^{2} &= \sum^{\alpha} \alpha = 1.
\end{align*}
Every integer is its own multiplicative inverse.
\end{solution}
\end{enumerate}
\end{problem}
\begin{problem}{2}
  Let $m$ be an integer, $m\geq 2$, and let $Z_{m}$ be the set of all positive integers less than $m$, $Z_{m}=\left\{ 0,1,\dots,m-1 \right\}$. If $\alpha$ and $\beta$ are in $Z_{m}$, let $\alpha + \beta$ be the least positive remainder obtained by dividing the (ordinary) sum of $\alpha$ and $\beta$ by $m$, and, similarly, let $\alpha\beta$ be the least remainder obtained by dividing the (ordinary) product of $\alpha$ and $\beta$ by $m$. (Example: if $m=12$, then $3+11=2$ and $3\cdot 11 = 9$.)
\begin{enumerate}
  \item Prove that $Z_{m}$ is a field if and only if $m$ is a prime.
\begin{proof}
  Let $m$ be a prime number. Note that addition and multiplication are both closed, commutative, associative and linear in the set of positive integers and so their least remainder when divided by $m$ is in $\Z_{m}$ (recall the equivalence classes in integers modulo $m$). Since $m\geq 2$, there is the additive identity $0$ and additive multiplicative $1$. Also, $0$ is its own additive inverse. Now consider some nonzero $\alpha \in Z_{m}$, then $m> m-\alpha>0$ and $m-\alpha \in Z_{m}$. Since $\alpha + (m-\alpha) = m = 0$, it follows that $m-\alpha$ is the unique additive inverse of $\alpha$.\\
  Now, we show for any nonzero $x\in Z_{m}$ that $x\cdot Z_{m} = Z_{m}$ (multiplication of all elements of $Z_{m}$ by $\alpha$) to prove the existence of some multiplicative inverse. Consider some nonzero $x \in Z_{m}$ and so $x = m-\beta$ where $\beta \in \left\{ 1,\dots,m-1 \right\}$. Note that there are only $m$ possible remainders when dividing a positive integer by $m$ and all are contained in $Z_{m}$. Hence, the only way $x\cdot Z_{m} \neq Z_{m}$ is when $|x\cdot Z_{m}|<|Z_{m}|$, namely, when there are distinct $y,z\in Z_{m}$ such that both $x\cdot y$ and $x\cdot z$ have the same remainder when divided by $m$. We show this is not possible when $m$ is a prime integer.\\
  Consider two distinct $y,z\in Z_{m}$ and so $y=m-\alpha$ and $z=m-\gamma$ for $\alpha,\gamma \in \left\{ 1,\dots,m \right\}$. Observe that 
\begin{align*}
  \Abs{(m-\beta)(m-\alpha) - (m-\beta)(m-\gamma)} &= \Abs{(m-\beta)(\gamma-\alpha)}.
\end{align*}
Since $y\neq z$, it follows that $\gamma \neq \alpha$ and so $\gamma-\alpha \neq 0$. Furthermore, $\Abs{\gamma-\alpha}< m$. Thus, $(m-\beta)(\gamma-\alpha)$ is the multiplication of two numbers that are not multiples of $m$ and so $\Abs{(m-\beta)(\gamma-\alpha)}$ is not a multiple of $m$ since $m$ is a prime number. Therefore, $m\nmid |y-z|$  and so $y$ and $z$ do not have equal remainder when divided by $m$. Thus, $x\cdot Z_{m}$ for any $x\in Z_{m}$ and so there is some $y\in Z_{m}$ such that $x\cdot y = 1$ (multiplicative inverse). This argument can be used to show that the multiplicative inverse for any nonzero $\alpha\in Z_{m}$ is unique.

For the converse, assume that $Z_{m}$ is a field. By \textbf{Problem 1}, for any elements $\alpha,\beta\in Z_{m}$, $\alpha\beta = 0$ if and only if at least one of them is $0$. This implies that all possible multiplications between the nonzero positive integers lower than $m$ are not multiples of $m$. Thus, $m$ is a factorization of itself times 1, $m$ must be a prime number.
\end{proof}
  \item What is $-1$ in $Z_{5}$
\begin{solution}
  Let the operations be extended to any integers, not just the ones inside $Z_{5}$. We know that $-5$ is a multiple of 5 and we must add $-4$ to $-1$ to get to $-5$. See this as some type of remainder, just as we need to add $-2$ to $2$ to get to $0$. Hence, $-1$ in $Z_{5}$ is 4.
\end{solution}
  \item What is $\frac{1}{3}$ in $Z_{7}?$
\begin{solution}
  It is not defined, since there is no integer in $\alpha\in Z_{7}$ such that $\frac{1}{3}-\alpha$ is divisible by $7$. 
\end{solution}
\end{enumerate}
\end{problem}
\begin{problem}{5}
  Let $Q(\sqrt{2})$ be the set of all real numbers of the form $\alpha + \beta\sqrt{2}$, where $\alpha$ and $\beta$ are rational.
\begin{enumerate}
  \item Is $Q(\sqrt{2})$ a field?
\begin{proof}
  Yes, it is a field. Note that $Q(\sqrt{2})\subseteq \R$ and so the properties of commutativity, associativity and linearity of multiplication and addition are present. Also, $\alpha+\beta\sqrt{2},-\alpha-\beta\sqrt{2}\in Q(\sqrt{2})$. Furthermore, $1+0\sqrt{2},0+0\sqrt{2}\in Q(\sqrt{2})$. We now show that addition and multiplication are closed. Consider some $\alpha +\beta\sqrt{2}$ and $\gamma + \epsilon\sqrt{2}$, where $\alpha,\beta,\gamma,\epsilon\in \Q$. Observe that
\begin{align*}
  \left( \alpha + \beta\sqrt{2} \right)+\left( \gamma +\epsilon\sqrt{2} \right)&= (\alpha+\gamma)+(\beta+\epsilon)\sqrt{2}\in \Q\left(\sqrt{2}\right)
\end{align*}
and 
\begin{align*}
  \left( \alpha + \beta\sqrt{2} \right)\left( \gamma +\epsilon\sqrt{2} \right)&= \alpha\gamma + \alpha\epsilon\sqrt{2} +\gamma\beta\sqrt{2} + 2\beta\epsilon\\
  &= (\alpha\gamma+2\beta\epsilon) + (\alpha\epsilon+\gamma\beta)\sqrt{2} \in \Q\left( \sqrt{2} \right)
\end{align*}
since $\Q$ is closed under multiplication and addition.\\

We just have to show that every nonzero element of $\Q\left( \sqrt{2} \right)$ has a unique inverse in $\Q\left( \sqrt{2} \right)$. Consider some $\alpha + \beta\sqrt{2} \in \Q\left( \sqrt{2} \right)$. If $\alpha = 0$ or $\beta =0$, then $\frac{1}{2\beta}\sqrt{2}$ and $\frac{1}{\alpha}$ are their inverses,respectively. If $\alpha,\beta\neq 0$, then $\frac{\alpha}{\alpha^{2}-2\beta^{2}}-\frac{\beta}{\alpha^{2}-2\beta^{2}}\sqrt{2}\in \Q\left( \sqrt{2} \right)$ is its inverse (Note that $\alpha^{2} - 2\beta^{2} = 0$ if and only if $|\alpha| = \sqrt{2}|\beta| \not\in \Q$). \\
Actually, we can use the same argument to show that $\Q\left( \sqrt{c} \right)$ is a field for any $c\in \Q$.
\end{proof}
  \item What if $\alpha$ and $\beta$ are required to be integers?
\begin{proof}
  Then it is not a field since not all members have a multiplicative inverse. For instance, consider $\alpha + 0\sqrt{2}\in \Z\left( \sqrt{2} \right)$ for some integer $|\alpha|>1$. Since $\alpha\in \Z$ its inverse is $\frac{1}{\alpha}$, however it is not an integer.
\end{proof}
\end{enumerate}
\end{problem}
\begin{problem}{6}
\begin{enumerate}
  \item Does the set of all polynomials with integer coefficitents form a field?
\begin{proof}
  No. The unique multiplicative identity is the polynomial $g(x) = 1$. We show that there is an infinity of polynomials in our set without inverse multiplicative. Consider some polynomial $p(x) = 0 + a_{1}x^{1} + a_{2}x^{2} + \dots + a_{n}x^{n}$ where $n\in \N$. Multiplying it by any other polynomial $s(x) = b_{0}+b_{1}x^{1}+\dots+b_{k}x^{k}$ for $k\in \N$, we get that
\begin{align*}
  p(x)s(x) &= \left( 0+a_{1}x^{1}+a_{2}x^{2}+\dots+a_{n}x^{n} \right)\left( b_{0}+b_{1}x^{1}+\dots+b_{k}x^{k} \right)\\
  &= 0\left( b_{0}+b_{1}x^{1}+\dots+b_{k}x^{k} \right) + a_{1}x^{1}\left( b_{0}+b_{1}x^{1}+\dots+b_{k}x^{k} \right)\\
  &+\dots+a_{n}x^{n}\left( b_{0}+b_{1}x^{1}+\dots+b_{k}x^{k} \right).
\end{align*}
The polynomial $p(x)s(x)$ is not a constant and so it can not be the multiplicative identity. 
\end{proof}
  \item What if the coefficients are allowed to be real numbers?
\begin{solution}
  The previous argument can still be used to show that it is not a field.  
\end{solution}
\end{enumerate}
\end{problem}
\begin{problem}{7}
  Let $\mathcal{F}$ be the set of all (ordered) pairs $(\alpha,\beta)$ of real numbers.
\begin{enumerate}
  \item If addition and multiplication are defined by 
\begin{align*}
  (\alpha,\beta)+(\gamma,\delta) = (\alpha+\gamma,\beta+\delta)
\end{align*}
and
\begin{align*}
  (\alpha,\beta)(\gamma,\delta) = (\alpha\gamma,\beta\delta),
\end{align*}
does $\mathcal{F}$ become a field?
\begin{proof}
  No, it is not a field. Some conditions may be fulfilled. For insatnce, since $\R$ is a field and the elements of the $j$'th place are added and multiplied, its easy to see that addition and multiplication is closed, commutative, associative and linear. However, since ther is no multiplicative inverse for 0, it follows that there is no ordered tuple $(\alpha,\beta)$ such that $(1,0)(\alpha,\beta) = (1,1)$. Hence, it is a nonzero element without multiplicative inverse.
\end{proof}
\item If addition and multiplication are defined by 
\begin{align*}
  (\alpha,\beta)+(\gamma,\delta) = (\alpha+\gamma,\beta+\delta)
\end{align*}
and
\begin{align*}
  (\alpha,\beta)(\gamma,\delta) = (\alpha\gamma-\beta\delta,\alpha\delta+\beta\gamma),
\end{align*}
is $\mathcal{F}$ a field then?
\begin{proof}
Yes, it is a field. Both addition and multiplication are closed since we are defining our operations as their application to the real elements inside the ordered tuples. Furthermore, as in the previous excercise, its easy to see why addition is associative and commutative. Also, for any scalar $(\alpha,\beta)$, $(\alpha,\beta)+(0,0) = (\alpha,\beta)$ and $(\alpha,\beta)+(-\alpha,-\beta) = (0,0)$. We show the required properties of multiplication. Consider the scalars $(\alpha,\beta), (\gamma,\delta)$ and $(\epsilon,\zeta)$\\
\begin{enumerate}[label= \arabic*.]
\item Commutativity:
\begin{align*}
  (\alpha,\beta)(\gamma,\delta) &= (\alpha\gamma-\beta\delta,\alpha\delta+\beta\gamma)\\
  &= (\gamma\alpha-\delta\beta,\delta\alpha+\gamma\beta) = (\gamma,\delta)(\alpha,\beta)\\
\end{align*}
\item Associativity:
\begin{align*}
  \left[(\alpha,\beta)(\gamma,\delta)\right](\varepsilon,\zeta) &= (\alpha\gamma-\beta\delta,\alpha\delta+\beta\gamma)(\varepsilon,\zeta)\\
  &= \alpha\gamma\varepsilon-(\beta\delta\varepsilon + \alpha\delta\zeta+\beta\gamma\zeta) \\
  &= (\alpha,\beta)(\gamma\varepsilon-\delta\zeta,\gamma\zeta+\delta\varepsilon) = (\alpha,\beta)\left[(\gamma,\delta)(\varepsilon,\zeta)\right]
\end{align*}
\item Multiplicative identity:\\
  Consider the scalar $(1,0)$. Then,
\begin{align*}
  (\alpha,\beta)(1,0) = (\alpha-0,0+\beta) = (\alpha,\beta)
\end{align*}
and so $(1,0)$ is the multiplicative identitity.
\item Multiplicative inverse:\\

  If either $\alpha = 0$ or $\beta = 0$, then the multplicative inverses are $(0,-1/\beta)$ and $(1/\alpha,0)$, respectively. On the other hand, if both $\alpha, \beta \neq 0$, then 
\begin{align*}
  \left( \frac{\alpha}{\alpha^{2}+\beta^{2}}, -\frac{\beta}{\alpha^{2}+\beta^{2}} \right)
\end{align*}
is the miltiplicative inverse.
\item Linearity:
\begin{align*}
  (\alpha,\beta)\left[ (\gamma,\delta)+(\epsilon,\zeta) \right] &= (\alpha,\beta)(\gamma+\epsilon, \delta+\zeta)\\
  &= (\alpha(\gamma+\epsilon) - \beta(\delta+\zeta), \alpha(\delta+\zeta)+\beta(\gamma+\epsilon))\\
&= ((\alpha\gamma-\beta\delta)+(\alpha\epsilon-\beta\zeta), (\alpha\delta+\beta\gamma)+(\alpha\zeta+\beta\epsilon))\\
  &= (\alpha,\beta)(\gamma,\delta) + (\alpha,\beta)(\epsilon,\zeta)
\end{align*}
\end{enumerate}
\end{proof}
\item What happens (in both the preceding cases) if we consider ordered pairs of complex numbers instead?
\begin{proof}
  Then $(a)$ is still not a field and $(b)$ is a field. This is so, since the properties of operations of real numbers are maintained for the complex field. Also, $0,1\in \mathbb{C}$.
\end{proof}
\end{enumerate}
\end{problem}

\section{Vector Space}

Just like fields, we have another type of set called the \textbf{Vector Space} with elements called \textbf{vectors} over some field $\mathcal{F}$. This set comes with two operations of addition and multiplication with the following properties:
\begin{enumerate}
  \item ADDITION: For any vectors $x$ and $y$ there is a vector $x+y$ in the space (closed under addition). 
\begin{enumerate}[label=\arabic*.]
  \item commutativity, $x+y=y+x$.
  \item associativity, $(x+y)+z=x+(y+z)$.
  \item additive identity, there is a unique vector $0$ such that  for every vector $x$, $x+0=x$.
  \item additive inverse, for every vector $x$, there is a unique vector $-x$ such that $x+(-x)=0$.\\

    As a comment, note that this are the same properties that addition has in the elemnts of a field. This is a similarity that both vector spaces and fields share: addition over their elements with these properties.\\
\end{enumerate}
\item MULTIPLICATION: This is not a multiplication between vectors but a scalar multplication, namely, for any vector $x$ and scalar $\alpha$ there is a vector $\alpha x$ in the space (closed under scalar multiplication).
  \begin{enumerate}[label=\arabic*.]
    \item associativity, $\alpha(\beta x) = (\alpha\beta)x$.
    \item $1x = x$ for every vector $x$\\

      Observe that multiplication is not defined between vectors but between a scalar and a vector. That's why multiplication can be interpreted as the application of operators on the elements of the vector space.
\end{enumerate}
\item CONNECTION: Just like in the properties of linearity for fields, vector spaces have properties that connect both structures of vector addition and scalar addition (field) in scalar multiplication (vector space).
\begin{enumerate}[label=\arabic*.]
  \item Scalar multiplication is distributive with respect to scalar addition, $(\alpha+\beta)x = \alpha x+ \beta x$
  \item Scalar multiplication is distributive with respect to  vector addition, $\alpha(x+y) = \alpha x + \alpha y$.
\end{enumerate}
\end{enumerate}
It's easy to see that any field $\mathcal{F}$ over itself is a vector space. Furthermore, we can extende this generalization to $\mathcal{F}^{n}$ for any $n\in \N$ by mathematical induction, if we define addition and scalar multiplication as the field addition and multiplication between the elements of the ordered $n$-tuples. For instance,
\begin{align*}
  (\alpha_{1},\alpha_{2},\dots,\alpha_{n}) + (\beta_{1},\beta_{2},\dots,\beta_{n}) = (\alpha_{1}+\beta_{1},\alpha_{2}+\beta_{2},\dots,\alpha_{n}+\beta_{n})
\end{align*}
and 
\begin{align*}
  \gamma(\alpha_{1},\alpha_{2},\dots,\alpha_{n}) =  (\gamma\alpha_{1},\gamma\alpha_{2},\dots,\gamma\alpha_{n}).
\end{align*}
Then,  $\R^{3}$ over $\R$ is a real vector space and $\mathbb{C}$ over $\mathbb{C}$ is a complex vector space.

Another interesting question is related to the minimum of elements a field and a vector space must have. The smallest field is $\mathcal{F} = \left\{ 0,1 \right\}$ and the smallest vector space is $V=\left\{ 0 \right\}$, since these are the only necessary elements in the properties. The other properties only discuss the characteristics of the operation.  
\begin{problem}{1}
  Prove that if $x$ and $y$ are vectors and if $\alpha$ is a scalar, then the following relations hold.
\begin{enumerate}
  \item $0+x=x$.
\begin{proof}
  We now that $x+0=x$. Since addition is commutative, $0+x=x+0=x$.
\end{proof}
\item $-0 = 0$.
\begin{proof}
  The additive inverse of $0$ is $-0$. Then, $0+(-0)=0$. However, we know that $0$ is the additive identity. Thus, $0+(-0) = -0 = 0$. The additive inverse of the additive identity is itself. This makes sense since the addition of $0$ with another nonzero vector gives the nonzero vector. 
\end{proof}
\item $\alpha \cdot 0 = 0$.
\begin{proof}
  Let's check whether multiplying $0$ by some scalar $\alpha$ changes its properties. Consider any $x$ and so $\alpha x$ is a vector. Then,
\begin{align*}
  \alpha\cdot 0 + \alpha\cdot x &= \alpha \cdot ( 0 + x)\\
  &= \alpha \cdot x.
\end{align*}
Thus, $\alpha\cdot 0 = 0$ (We can add the additive inverse of $\alpha x$ to both sides). Thishas to do with the uniqueness of the additive identity.
\end{proof}
\item $0\cdot x = 0$. (The symbol $0$ on the left of scalar multiplication denotes a scalar, on the right it denotes a vector).
\begin{proof}
  Simply apply the distributive property with respect to scalar addition, namely, 
\begin{align*}
  0\cdot x + \alpha \cdot x &= (0+\alpha)\cdot x\\
  &= \alpha \cdot x.
\end{align*}
Then, $0\cdot x$. Thus, the scalar $0$ can be seen as some operator that neutralizes the vector. 
\end{proof}
\item If $\alpha x = 0$, then either $\alpha = 0$ or $x=0$ (or both).
\begin{proof}
  Suppose that $\alpha = 0$. Then, the result is true. On the other hand, assume that $\alpha \neq 0$. Consider some nonzero vector $c$. Then, 
\begin{align*}
  \alpha\cdot x + \alpha\cdot c &= \alpha\cdot (x+c)\\
  &= \alpha\cdot c.
\end{align*}
Then, Multiplying both sides by the inverse of $\alpha$, $x+c = c$. Thus, $x=0$.
\end{proof}
  \item $-x=(-1)x$.
\begin{proof}
  Note that 
\begin{align*}
  x + (-x ) &= 0 \\
  &= (1+(-1))\cdot x \\
  &= x + (-1)\cdot x. 
\end{align*}
Then, $-x = (-1)\cdot x$.
\end{proof}
  \item $y+(x-y) = x$. (Here $x-y=x+(-y)$.)
\begin{proof}
Note that
\begin{align*}
  y+(x-y) &= y+(-y+x) \\
  &= (y-y)+x = 0+x.
\end{align*}
\end{proof}
\end{enumerate}
\end{problem}
\begin{problem}{2}
  If $p$ is a prime, then $\Z_{p}^{n}$ is a vector space over $\Z_{p}$; how many vectors are there in this vector space?
\begin{solution}
  Because a vector space is closed under addition and scalar multiplication we consider the cardinality of $\Z_{p}^{n}$. We know that $\Z_{p}$ contains $p$ elements and there is a bijection from $\Z_{p}^{n}$ to all possible permutations with $n$ elements of $\Z_{p}$. Hence, the cardinality of the vector space in question is $p^{n}$. 
\end{solution}
\end{problem}
\begin{problem}{3} 
  Let $\mathcal{V}$ be the set of all (ordered) pairs of real numbers. If $x=(\xi_{1},\xi_{2})$ and $y=(\eta_{1},\eta_{2})$ are elements of $\mathcal{V}$, write
\begin{align*}
  x+y &= (\xi_{1}+\eta_{1}, \xi_{2} + \eta_{2})\\
  \alpha x &= (\alpha\xi_{1}, 0)\\
  0 &= (0,0)\\
  -x &= (-\xi_{1},-\xi_{2}).
\end{align*}
Is $\mathcal{V}$ a vector space with respect to these definitions of linear operations? Why?
\begin{solution}
  The set $\mathcal{V}$ over $\R$ is not a vector space with respect to thes definitions of operations. Note that the scalar $1$ is no longer a mutltiplicative identity with respect to scalar multiplication because  
\begin{align*}
  1x &= (1\cdot\xi_{1},0) \neq (\xi_{1},\xi_{2}) = x
\end{align*}
\end{solution}
\end{problem}
\begin{problem}{4}
  Sometimes a subset of a vector space is itself a vector space (with respect to the linear operations already given). Consider, for example, the vector space $\mathbb{C}^{3}$ and the subsets $\mathcal{V}$ of $\mathbb{C}^{3}$ consisting of those vectors $(\xi_{1},\xi_{2},\xi_{3})$ for which 
\begin{enumerate}
  \item $\xi_{1}$ is real,
  \item $\xi_{1} = 0$.
  \item either $\xi_{1}=0$ or $\xi_{2}=0$,
  \item $\xi_{1}+\xi_{2} = 0$,
  \item $\xi_{1}+\xi_{2}=1$.
\end{enumerate}
In which of these cases is $\mathcal{V}$ a vector space?
\begin{solution}
  Note that the set $\mathcal{V}\subseteq \mathbb{C}^{3}$ over $\mathbb{C}$ has the same linear operations with their required properties for a vector space. Hence, we must only check if $\mathcal{V}$ is closed under these linear operations, each vector has its respective inverse and $0\in \mathcal{V}$ to conclude whether it is a vector space or not. \\
  Then, (a), (c) and (e) are not vector spaces since 
\begin{align*}
  \alpha\cdot (\xi_{1},\xi_{2},\xi_{3}) &= (\alpha\xi_{1},\alpha\xi_{2},\alpha\xi_{3}) \not\in \mathcal{V}, \text{ for }\alpha\in \mathbb{C}/\R\; \text{ (not closed)},\\
  (\xi_{1},0,\xi_{3}) + (0,\gamma_{2},\gamma_{3}) &= (\xi_{1},\gamma_{2},\gamma_{3}+\xi_{3}) \not\in \mathcal{V}, \text{ for }\xi_{1},\gamma_{2}\neq 0\; \text{ (not closed)} \text{ and}\\
  (\xi_{1},\xi_{2},\xi_{3}) + (\gamma_{1},\gamma_{2},\gamma_{3}) &= (\xi_{1} + \gamma_{1}, \xi_{2} + \gamma_{2}, \xi_{3} + \gamma_{3})\not\in \mathcal{V}, \text{ because }\xi_{1}+\gamma_{1}+\xi_{2}+\gamma_{2} = 2 \;\\
  \text{ (not closed)},
\end{align*}
respectively.\\
On the other hand, $(0,0,0)\in \mathcal{V}_{b},\mathcal{V}_{d}$, where $\mathcal{V}_{x}$ is the subset of case $x$. Furtheremore, for $\mathcal{V}_{b}$,  
\begin{align*}
  (0,\xi_{2},\xi_{3}), (0,-\xi_{2},-\xi_{3}) &\in\mathcal{V}_{b}\\ 
  \alpha\cdot (0,\xi_{2},\xi_{3}) &= (\alpha0,\alpha\xi_{2},\alpha\xi_{3})\in \mathcal{V}_{b} \text{ and }\\
  (0,\xi_{2},\xi_{3}) + (0,\gamma_{2},\gamma_{3}) &=  (0+0,\xi_{2}+\gamma_{2},\xi_{3}+\gamma_{3})\in \mathcal{V}_{b}.
\end{align*}
In the case of $\mathcal{V}_{d}$, note that each vector  $(\xi_{1},\xi_{2},\xi_{3}) = (\xi_{1},-\xi_{1},\xi_{3})$ due to the uniqueness of the additive inverse in the field $\mathbb{C}$, and so 
\begin{align*}
  (\xi_{1},-\xi_{1},\xi_{3}),(-\xi_{1},\xi_{1},-\xi_{3})&\in \mathcal{V}_{d}\\
  \alpha\cdot (\xi_{1},-\xi_{1},\xi_{3}) &= (\alpha\xi_{1},-\alpha\xi_{1},\alpha\xi_{3})\in \mathcal{V}_{b} \;\text{ and }\\
  (\xi_{1},-\xi_{1},\xi_{3}) + (\gamma_{1},-\gamma_{1},\gamma_{3}) &=  (\xi_{1}+\gamma_{1},-(\xi_{1}+\gamma_{1}),\xi_{3}+\gamma_{3})\in \mathcal{V}_{b}.
\end{align*}
\end{solution}
\end{problem}
\begin{problem}{5}
  Consider the vector space $\mathcal{P}$ and the subsets $\mathcal{V}$ of $\mathcal{P}$ consisting of those vectors (polynomials) $x$ for which
\begin{enumerate}
  \item $x$ has degree 4,
  \item $2x(0) = x(1)$,
  \item $x(t)\geq 0$ whenever $0\leq t \leq 1$,
  \item $x(t) = x(1-t)$ for all $t$.
\end{enumerate}
In which of these cases is $\mathcal{V}$ a vector space?
\begin{solution}
  Just like in the previous excercise, $\mathcal{V}$ has the same linear operations of addition and scalar multiplication as the vector space $\mathcal{P}$ over $\R$. Hence, for the subset $\mathcal{V}_{x}$ of case $x$, it suffices to check if  $0\in \mathcal{V}_{x}$, each vector has its unique inverse and $\mathcal{V}_{x}$ is closed under this linear operations to conclude that it is a vector space. Here, $0$ is the polynomial $p(t)=0$ for all $t\in \R$.
  Then, (a) and (c) are not vector spaces because 
\begin{align*}
0 &\not\in \mathcal{V}_{a} \text{ (no additive identity) since 0 has no degree 4 and}\\
  \text{if }x(t)&\in\mathcal{V}_{c}\text{ and }x(1)>0,\text{ then } -x(t)\not\in \mathcal{V}_{c} \text{ (no additive inverse)} 
\end{align*}
since $-x(1) = (-1)\cdot x(1)<0$. On the other hand, $0\in \mathcal{V}_{b},\mathcal{V}_{d}$ since $2\cdot p(0) = p(1) = 0$ and $p(t) = p(1-t)=0$ for all $t\in \R$. Furthermore, if $x(t) \in \mathcal{V}_{b}$, then $2\cdot(-x(0)) = (2(-1))\cdot x(0) = -1\cdot(2\cdot x(0)) = (-1)\cdot x(1) = -x(1)$ and so $-x(t)\in \mathcal{V}_{b}$. Also, for some $x_{1},x_{2}\in \mathcal{V}_{b}$ and $\alpha\in\R$,
\begin{align*}
  2\cdot(x_{1}+x_{2})(0) &= 2\cdot(x_{1}(0) + x_{2}(0)) = x_{1}(1) +x_{2}(1)\\
  &= (x_{1}+x_{2})(1)
\end{align*}
and 
\begin{align*}
  2\cdot(\alpha\cdot x_{1}(0)) &= (\alpha 2)\cdot x_{1}(0) = \alpha\cdot (2\cdot x_{1}(0))\\
  &= \alpha \cdot x(1).
\end{align*}
Now consider some $x_{1},x_{2}\in \mathcal{V}_{d}$ and $\alpha\in \R$. Then, $-x(t) = (-1)\cdot x(t) = (-1)\cdot x(1-t) = -x(1-t)$ for every $t\in \R$. Moreover,  
\begin{align*}
  (x_{1}+x_{2})(t) &= x_{1}(t) + x_{2}(t) = x_{1}(1-t) + x_{2}(1-t)\\
  &= (x_{1}+x_{2})(1-t) 
\end{align*}
and
\begin{align*}
  \alpha\cdot x_{1}(t) &= \alpha \cdot x_{1}(1-t)
\end{align*}
for any $t\in\R$. Hence, $\mathcal{V}_{b}$ and $\mathcal{V}_{d}$ are vector spaces.
\end{solution}
\end{problem}

\section{Properties of sets of vectors: Linear dependence and Linear combinations}

\subsection{Linear dependence}

Now that we have described these sets with specific characteristics known as \textbf{Vector Spaces} over some field, we can explore some interesting properties that their subsets can have. But first, let's define the summation notation $\sum_{i} x_{i}$ for vectors of a vector space $\mathcal{V}$. The set $\left\{ x_{i} \right\}$ is a subset of $\mathcal{V}$ where $i\in I$ are the indexes. Then, 
\begin{align*}
  \sum_{i} x_{i} = x_{i_{1}} + x_{i_{2}} + \dots x_{i_{n}}
\end{align*}
namely, the sequent application of addition for the vectors $\left\{ x_{i} \right\}$. Furthermore, if $I = \emptyset$, then $\left\{ x_{i} \right\}= \emptyset$ and $\sum_{i}x_{i}$ is defined to be the vector $0$ since there are no indexes to assign to vectors.\\

Now, we can proceed with the discussion of linear dependence. A finite subset $\left\{ x_{i} \right\}\subseteq \mathcal{V}$ is linearly dependent if there exists a set $\left\{ \alpha_{i} : \exists i, \alpha_{i} \neq 0\right\}$ of scalars such that $\sum_{i} \alpha_{i}x_{i} = 0$. On the other hand, by negation, it is linearly independent if no such set exists. This can be interpreted as that for every set $\left\{ \alpha_{i}: \exists i, \alpha_{i}\neq 0 \right\}$ of scalars, $\sum_{i} \alpha_{i}x_{i} \neq 0$. Because we are dealing with scalars of a field and $0x = 0$, it is true that, in the case of a linearly independent set $\left\{ x_{i} \right\}$,   
\begin{align*}
  \sum_{i} \alpha_{i}x_{i} =0 \iff \forall i\in I, \alpha_{i} = 0.
\end{align*}
One can push the limits of this ideas and ask in which category does the empty set $\emptyset \subseteq \mathcal{V}$ resides. After all, it is a finite subset of $\mathcal{V}$. The issue with this, is that $\left\{ x_{i} \right\} = \emptyset$ implies that $\left\{ i \right\}=\emptyset$, namely, we don't have indexes to assign to something since there are not things to be assigned (vectors). However, note that  
\begin{align*}
  \left\{ i \right\}=\emptyset \implies \sim \left( \exists \left\{ \alpha_{i}:\exists i, \alpha_{i} \neq 0 \right\}, \sum_{i}\alpha_{i} x_{i} = 0 \right)
\end{align*}
since we don't have indexes to assign to scalars and so there is no $i$ such that $\alpha_{i} \neq 0$. This implies that no such set $\left\{ \alpha_{i}: \exists i, \alpha\neq 0 \right\}$ can be constructed. Hence, by definition, the empty set is linearly independent. Obviously, this argument uses the excluded middle principle, and is open for debate.\\
In the case of not finite set $\mathcal{M}$ of vectors, we say that $\mathcal{M}$ is linearly dependent if every finite subset is linearly dependent.\\

Recall that every field over itself is a vector space. Let $\mathcal{V}$ be an example of such vector space. Then, consider the set $S=\{x,y\}\subseteq \mathcal{V}$. If $x=y=0$, then $S$ is linearly dependent. If not, $(-x)y + (y)x = 0$. Hence, every subset of $\mathcal{V}$ containing two vectors is linearly dependent. Using mathematical induction, one can conclude that any finite subset such that $|S|\geq 2$ is linearly dependent. However, $\mathcal{V}$ is not linearly dependent, since $\left\{ x \right\}$ for any nonzero $x\in\mathcal{V}$ is linearly independent. This has to do with the uniqueness of $0$ as the additive identitiy.\\

Furthermore, a vector $x$ is a linear combination of the set $\left\{ x_{i} \right\}$ if $x = \sum_{i}\alpha_{i}x_{i}$. Note that, by definition, $0$ is linearly dependent on the emptyset. It is common to say that $x$ is linearly dependent on $\left\{ x_{i} \right\}$. This is so due to the following thorem. 
\begin{theorem}{1}
  Let $x$ be a vector in the vector space $\mathcal{V}$ and $\left\{ x_{i} \right\}\subseteq \mathcal{V}$ be linearly independent. Then, the set $\left\{ x \right\}\cup\left\{ x_{i}x \right\}$ is linearly dependent if and only if $x=\sum_{i}\alpha_{i}x_{i}$.
\begin{proof}
  Assume that $\left\{ x_{i} \right\}\cup \left\{ x \right\}$ is linearly dependent. Hence, there is a set $S_{j} = \left\{ \alpha_{j}:\exists j,\alpha_{j}\neq 0 \right\}$ such that $\sum_{j} \alpha_{j} x_{j} = 0$, here $j$ is the new index notation for $\left\{ x_{i} \right\}\cup \left\{ x \right\}$. Since $\left\{ x_{i} \right\}$ is linearly independent, it follows that the coefficient $\alpha$ of $x$ can not be 0 since there is no set $S_{i}$ such that $0x + \sum_{i}\alpha_{i}x_{i} = 0$. Hence, 
\begin{align*}
  x = \alpha^{-1} \left(\sum_{i}-\alpha_{i}x_{i}\right),
\end{align*}
and so $x$ is a linear combination of $\left\{ x_{i} \right\}$. Note that the scope of the argument includes the case when $\left\{ x_{i} \right\}=\emptyset$. For the converse, assume that $x = \sum_{i}\alpha_{i}x_{i}$ and so $(1)x-\sum_{i}\alpha_{i}x_{i} = 0$. Therefore, $1\in \left\{ \alpha_{j} \right\} \supseteq \left\{ \alpha_{i} \right\}$ and the set $\left\{ x_{j} \right\} = \left\{ x_{i} \right\}\cup \left\{ x \right\}$ is linearly dependent. 
\end{proof}
\end{theorem}
Additionaly, a set of non-zero vectors $\left\{ x_{n} \right\}$ is linearly dependent if and only if some $x_{k}$, $2\leq k \leq n$, is a linear combination of the preceding ones.
Another concept that was explained is basis. Basically, a basis is a linearly independent subset of some vector space such that any vector is a linear combination of it. Interestingly, for any finite vector space, one can extend any linearly independent subset to a basis by adding vectors to it. In the case of the vector space $\left\{ 0 \right\}$, its basis is the empty set. Also, a vector space with a finite basis is known as a \textbf{finite-dimensional vector space}. 
\begin{problem}{1}
\begin{enumerate}
  \item Prove that th four vectors
\begin{align*}
  x &= (1,0,0)\\
  y &= (0,1,0)\\
  z &= (0,0,1)\\
  u &= (1,1,1),
\end{align*}
in $\mathcal{C}^{2}$ form a linearly dependent set, but any three of them are linearly independent.
\begin{proof}
  Let $S=\left\{ x,y,z,u \right\}$. Note that $u= x+y+z = (1,1,1)$ and so $u-(x+y+z) = 0$ and so the set of scalars $\left\{ \alpha_{i} \right\}$ contains nonzero elements. Hence, $S$ is linearly dependent.\\
  Now, consider some subset $P\subseteq S$ with only three elements. Since three of the four vectors have only one nonzero element in one unique place of the ordered triple, it follows that $P$ contains at least two of this vectors. Then, this two vectors coincide in having a zero element in the $i$'th place of the ordered triple, and the third vector has a nonzero element in place $i$. Hence, $0\cdot \gamma_{i} = 0$, being $\gamma_{i}$ the $i$'th element of the third vector, implies that $\alpha_{3} = 0$. Furthermore, the first two mentioned vectors do not coincide in their nonzero elements and so $\alpha_{1} x +\alpha_{2} y =0 \implies \alpha_{1},\alpha_{2}=0$. Therefore, $P$ is linearly independent.
\end{proof}
\item If the vectors $x, y ,z,$ and $u$ in $\mathcal{P}$ are defined by $x(t) = 1$, $y(t) = t$, $z(t) =t^{2}$, and $u(t) = 1+t+t^{2}$, prove that $x,y,z,$ and $u$ are linearly dpenden, but any three of them are linearly dependent.
\begin{proof}
  Let $S=\left\{ x,y,z,u \right\}$. Observe that $u(t) = 1+t+t^{2} = (1)x(t) + (1)y(t) + (1)z(t)$. Hence, $u$, a nonzero vector, is a linear combination of $x,y,z$ and so $S$ is a linearly dependent set.
  Note that 
\begin{align*}
  x(t) = 1+0t+0t^{2} &\mapsto (1,0,0)\\
  y(t) = 0+t+0t^{2} &\mapsto (0,1,0)\\
  z(t) = 0+0t+t^{2} &\mapsto (0,0,1)\\
  u(t) = 1+t+t^{2} &\mapsto (1,1,1),
\end{align*}
which is the same set of vectors that we saw earlier. Hence, by the same argument, any subset $P\subset S$ of three vectors is linearly independent.
\end{proof}
\end{enumerate}
\end{problem}
\begin{problem}{2}
  Prove that if $\R$ is considered as a rational vector space (a vector space over $\Q$), then a necessary and sufficient condition that the vectors 1 and  $\xi$ in $\R$ be linearly independent is that the real number $\xi$ be irrational.
\begin{proof}
  Suppose that $\xi$ is irrational. Then, $\alpha\cdot 1 + \beta\cdot \xi$, where $\alpha,\beta\in \Q$ and $\beta \neq 0$, is an irrational number and thus nonzero. If $\beta = 0$, then $\alpha\cdot 1$ is zero if and only if $\alpha = 0$. Therefore, $\left\{ 1,\xi \right\}$ is a linearly independent set.\\
  For the converse, assume that $\left\{ 1,\xi \right\}$ is linearly independent. We know that $\xi$ being irrational and $\left\{ 1,\xi \right\}$ is linearly independent are both true. Then, we only have to show that this is not the case when $\xi$ is rational. Suppose that $\xi$ is rational. If $\xi = 0$, then $0\cdot 1 + 3\cdot 0 = 0$. On the other hand, $\xi\cdot 1 + (-1)\cdot \xi = 0$ for any nonzero $\xi$.
\end{proof}
\end{problem}
\begin{problem}{3}
  Is it true that if $x, y,$ and $z$ are linearly independent vectors, then so also are $x+y$, $y+z,$ and $z+x$?
\begin{proof}
  Yes, the set $A=\left\{ x+y,y+z,z+x \right\}$ is linearly independent. First, consider any field $\mathcal{B}$. Let $\left\{ \alpha,\beta,\gamma \right\}\subseteq \mathcal{B}$, where at least one of them is nonzero. Then,
\begin{align*}
  \alpha (x+y) + \beta (y+z) + \gamma (z+x) &= (\alpha+\gamma)x + (\alpha+\beta)y + (\beta+\gamma)z.
\end{align*}
Since one of the scalars is nonzero, it follows that two scalars must be the inverse additive of this nonzero scalar for two of the sums to be zero, which implies that their sum is nonzero. Hence, at least one of the sums $\alpha+\beta, \alpha+\gamma,\gamma + \beta$ is nonzero. Thus,  $(\alpha+\gamma)x + (\alpha + \beta) y + (\beta+\gamma)z \neq 0$ (recall that $\left\{ x,y,z \right\}$ is linearly independent).
\end{proof}
\end{problem}
\begin{problem}{4}
\begin{enumerate}
  \item Under what conditions on the scalar $\xi$ are the vectors $(1+\xi, 1-\xi)$ and $(1-\xi,1+\xi)$ in $\mathbb{C}^{2}$ linearly dependent?
\begin{proof}
  The vectors are linearly dependent if and only if $\xi = 0$. Clearly, the vectors are equal when $\xi = 0$ and so linearly dependent. For the converse, assume that the vectors are linearly dependent. Hence, there are scalars $\alpha$ and $\beta$ such that $\alpha(1+\xi,1-\xi) + \beta(1-\xi,1+\xi)=0$, where at least one of them, say $\alpha$, is nonzero. Then,
\begin{align*}
  (\alpha+\beta+(\alpha-\beta)\xi, \alpha+\beta-(\alpha-\beta)\xi)=0.
\end{align*}
Hence, $\alpha+\beta = -(\alpha-\beta)\xi$ and so $2(\alpha+\beta)=0$. This further implies that $\alpha = -\beta\neq 0$. Thus, $-(-2\beta)\xi=0$ and so $\xi =0$.
\end{proof}
  \item Under what conditions on the scalar $\xi$ are the vectors $(\xi,1,0),(1,\xi,1)$ and $(0,1,\xi)$ in $\R^{3}$ linearly dependent?
\begin{proof}
  Let these vectors be linearly dependent. Then, there are some scalars $\alpha,\beta,\gamma\in \R$ such that 
\begin{align*}
  \alpha (\xi,1,0) + \beta(1,\xi,1) + \gamma(0,1,\xi) = 0,
\end{align*}
where at least one of them is nonzero. Then,
\begin{align*}
  \alpha\xi = \gamma \xi = -\beta& &\text{and}& &\alpha+\gamma+\beta\xi = 0.
\end{align*}
If $\xi = 0$, then $\beta = 0$ and $\alpha=-\gamma \neq 0$. Now, let's consider the case when $\xi \neq 0$. Then, the left equation implies that $\gamma = \alpha = -\beta/\xi$ and so $\beta\neq 0$ (at least one of the scalars is nonzero). Futhermore,  
\begin{align*}
  \alpha + \gamma + \beta\xi &= 2\gamma + \beta\xi \\
  &= -2\beta/\xi + \beta\xi = 0.
\end{align*}
With algebraic manipulations, one can conclude that $|\xi| = \sqrt{2}$.
\end{proof}
  \item What is the answer to (b) for $\Q^{3}$ (in place of $\R^{3}$)?
\begin{proof}
  From (b), it is evident that the vectors are linearly dependent if and only if $\xi = 0,\pm \sqrt{2}$. Hence, the vectors are linearly dependent in $\Q^{3}$ if and only if $\xi = 0$ since $\Q^{3}\subset \R^{3}$. 
\end{proof}
\end{enumerate}
\end{problem}
\begin{problem}{5}
\begin{enumerate}
  \item The vectors $(\xi_{1},\xi_{2})$ and $(\eta_{1},\eta_{2})$ in $\mathbb{C}^{2}$ are linearly dependent if and only if $\xi_{1}\eta_{2} = \xi_{2}\eta_{1}$.
\begin{proof}
  Assume that $\eta_{2}\xi_{1} = \xi_{2}\eta_{1}$. Then, 
\begin{align*}
  \eta_{1} (\xi_{1},\xi_{2}) - \xi_{1}(\eta_{1},\eta_{2}) &= (\eta_{1}\xi_{1}-\xi_{1}\eta_{1}, \eta_{1}\xi_{2}-\xi_{1}\eta_{2})\\
  &= 0.
\end{align*}
For the converse, suppose that the vectors are linearly dependent. Then, one can be expressed as the other multiplied by a constant (since they are linearly dependent, the two scalars must be nonzero so that the sum of the vectors multiplied by their respective scalar is zero). Hence, $(\eta_{1},\eta_{2}) = c(\xi_{1},\xi_{2})$. Thus, $\eta_{1} = c\xi_{1}$ and $\eta_{2} = c\xi_{2}$. Multiplying the left equality by $\eta_{2}$, we get $c\xi_{2}\eta_{1} = c\xi_{1}\eta_{2}$. Because $c\neq 0$, $\eta_{1}\xi_{2} = \xi_{1}\eta_{2}$. 
\end{proof}
  \item Find a similar necessary and sufficient condition for the linear dependence of two vectors in $\mathbb{C}^{3}$. Do the same for three vectors in $\mathbb{C}^{3}$.
\begin{solution}
  Two vectors $(\xi_{1},\xi_{2},\xi_{3})$ and $(\eta_{1},\eta_{2},\eta_{3})$ are linearly dependent if and only if $\xi_{i}\eta_{j} = \eta_{i}\xi_{j}$ for any $i,j\in \left\{ 1,2,3 \right\}$. 
\begin{proof}
  Assume that $\xi_{i}\eta_{j} = \eta_{i}\xi_{j}$ for any $i,j\in \left\{ 1,2,3 \right\}$. Then, for some $i\in \left\{ 1,2,3 \right\}$, 
\begin{align*}
  \eta_{i}(\xi_{1},\xi_{2},\xi_{3}) - \xi_{i}(\eta_{1},\eta_{2},\eta_{3}) &= (\eta_{i}\xi_{1} - \xi_{i}\eta_{1}, \eta_{i}\xi_{2} - \xi_{i}\eta_{2}, \eta_{i}\xi_{3} - \xi_{i}\eta_{3})\\
  =0.
\end{align*}
For the converse, suppose that these vectors are linearly dependent. Then, $\xi_{1} = c\eta_{1}, \xi_{2} = c\eta_{2}$ and $\xi_{3} = c\eta_{3}$. Generally, we have $\xi_{i} = c\eta_{i}$ for any $i\in \left\{ 1,2,3 \right\}$. Then, for some $j\in \left\{ 1,2,3 \right\}$, $c\eta_{j}\xi_{i} = c\eta_{i}\xi_{j}$. Since $c\neq 0$, it follows that $\eta_{j}\xi_{i} = \eta_{i}\xi_{j}$ for any $i,j\in \left\{ 1,2,3 \right\}$.
\end{proof}
For any $3$ vectors $(a_{1},b_{1},c_{1}),(a_{2},b_{2},c_{2}),(a_{3},b_{3},c_{3})\in \mathcal{C}^{3}$, they are linearly dependent if and only if 
\begin{align*}
  a_{1}b_{3}c_{2} + a_{3}b_{2}c_{1} + a_{2}b_{1}c_{3} = a_{2}b_{3}c_{1} + a_{1}b_{2}c_{3} + a_{3}b_{1}c_{2}.
\end{align*}
(Note the ones on the left have indexes ordered in an improper permutation, the opposite is seen on the right-side).
\begin{proof}
  First, assume the latter for some set of linearly independent vectors in $\mathbb{C}^{3}$. Then, consider the following linear combination, 
\begin{align*}
  \left( \left[ b_{2}c_{3}-b_{3}c_{2} \right] + \left[ a_{3}c_{2}-a_{2}c_{3} \right]+\left[ a_{2}b_{3}-a_{3}b_{2} \right] \right) \cdot (a_{1},b_{1},c_{1})+\\
  \left( \left[ b_{3}c_{1}-b_{1}c_{3} \right] + \left[ a_{1}c_{3}-a_{3}c_{1} \right]+\left[ a_{3}b_{1}-a_{1}b_{3} \right] \right) \cdot (a_{2},b_{2},c_{2})+\\
  \left( \left[ b_{1}c_{2}-b_{2}c_{1} \right] + \left[ a_{2}c_{1}-a_{1}c_{2} \right]+\left[ a_{1}b_{2}-a_{2}b_{1} \right] \right) \cdot (a_{3},b_{3},c_{3})
\end{align*}
For the converse, suppose that these vectors are linearly dependent. Then, there are scalars $a,b,c \in \mathbb{C}$ such that $a x_{1} + b x_{2} + c x_{3} =0$ and at least one of them, say $\alpha$, is nonzero. Hence,
\begin{align*}
  a_{1} &= \beta a_{2} + \gamma a_{3}\\
  b_{1} &= \beta b_{2} + \gamma b_{3}\\
  c_{1} &= \beta c_{2} + \gamma c_{3}.
\end{align*}
If only the scalar $a$ is nonzero, then $x_{1}=0$ (the vectors are linearly dependent) and so $0b_{3}c_{2} + 0b_{2}c_{1} + 0b_{1}c_{3} = 0 = 0b_{3}c_{1} + 0b_{2}c_{3} + 0b_{1}c_{2}$. If only two are nonzero, say $a$ and $b$, then, by the previous result for a linearly dependent set of two vectors, $a_{1}(b_{3}c_{2} - b_{2}c_{3}) + a_{3}(b_{2}c_{1} - b_{1}c_{2}) + a_{2}(b_{1}c_{3} - b_{3}c_{1}) = (a_{1}+a_{2}+a_{3})0 = 0$. Thus, we can assume that all scalars are nonzero and so $\beta,\gamma \neq 0$.\\
Now, note that $a_{1}(\beta b_{2} + \gamma b_{3}) = b_{1} (\beta a_{2} + \gamma a_{3})$ and so 
\begin{align}
  \gamma c_{3}(\beta a_{1}b_{2} -\beta a_{2}b_{1}) &= c_{1}(\gamma a_{3}b_{1} - \gamma a_{1}b_{3}) - \beta c_{2} (\gamma a_{3}b_{1} - \gamma a_{1}b_{3})\\
  \gamma\beta(a_{1}b_{2}c_{3} - a_{2}b_{1}c_{3}) &= - \gamma \beta (a_{3}b_{1}c_{2} - a_{1}b_{3}c_{2}) + \gamma a_{3}b_{1}c_{1} - \gamma a_{1}b_{3}c_{1}.
\end{align}
Let's find some useful equalities for $\gamma a_{1}b_{3}c_{1}$ and $\gamma a_{3}b_{1}c_{1}$ for substitution in the previous equality. Note that   
\begin{align*}
  \gamma\beta(a_{1}c_{2}b_{3} - a_{2}c_{1}b_{3}) &= - \gamma \beta (a_{3}c_{1}b_{2} - a_{1}c_{3}b_{2}) + \gamma a_{3}c_{1}b_{1} - \gamma a_{1}c_{3}b_{1}\\
  \gamma\beta(c_{1}b_{2}a_{3} - c_{2}b_{1}a_{3}) &= - \gamma \beta (c_{3}b_{1}a_{2} - c_{1}b_{3}a_{2}) + \gamma c_{3}b_{1}a_{1} - \gamma c_{1}b_{3}a_{1}
\end{align*}
by interchanging $b$ and $c$, and $a$ and $c$ in equation (2), respectively. It's like using the same algorithm to derive (2) but interchanging the position of letters (placeholders). Hence,
\begin{align*}
  \gamma\beta (a_{1}b_{2}c_{3} - a_{2}b_{1}c_{3} + a_{3}b_{1}c_{2} - a_{1}b_{3}c_{2}) &= \gamma\beta (a_{1}c_{2}b_{3}-a_{2}c_{1}b_{3}+a_{3}c_{1}b_{2}-a_{1}c_{3}b_{2}) + \gamma a_{1}c_{3}b_{1}\\
  + \gamma\beta (c_{1}b_{2}a_{3} - c_{2}b_{1}a_{3} + c_{3}b_{1}a_{2} - c_{1}b_{3}a_{2}) - \gamma c_{3}b_{1}a_{1} &\implies\\
  2\gamma\beta (a_{1}b_{2}c_{3} + a_{3}b_{1}c_{2} + a_{2}b_{3}c_{1}) &= 2\gamma\beta (a_{2}b_{1}c_{3} +a_{1}b_{3}c_{2}+a_{3}b_{2}c_{1}).  
\end{align*}
Therefore,  
\begin{align*}
  a_{1}b_{3}c_{2} + a_{3}b_{2}c_{1} + a_{2}b_{1}c_{3} = a_{2}b_{3}c_{1} + a_{1}b_{2}c_{3} + a_{3}b_{1}c_{2}.
\end{align*}
\end{proof}
\end{solution}
  \item Is there a set of three linearly independent vectors $\mathbb{C}^{2}$?
\end{enumerate}
\end{problem}
\begin{problem}{8}
\begin{enumerate}
  \item Under what conditions on the scalar $\xi$ do the vectors $(1,1,1)$ and $(1,\xi,\xi^{2})$ form a basis of $\mathbb{C}$?
\begin{solution}
  We know that for any $n\in\N$, the basis of $\mathbb{C}^{n}$ must contain at least $n$ vectors. Furthermore, these must be linearly independent. Otherwise,
\begin{align*}
  x_{1} = \alpha_{2}x_{2}+\alpha_{3}x_{3}+\dots+\alpha_{n}x_{n}
\end{align*}
for some $x_{1}$ and scalars $\alpha_{2},\dots,\alpha_{n}$, and so the linear combination $\sum_{i=1}^{n}\beta_{i}x_{i} = \sum_{i=2}^{n}(\alpha_{i}\beta_{1} + \beta_{i})x_{i}$ is a linear combination of $n-1$ vectors.\\

Then, since there are two vectors and the vector space is $\mathbb{C}$, it follows that we just have to look for a condition for them to be lineraly independent. Then, this vectors form a basis if and  only if $\xi \neq 1$. 
\begin{proof}
  Assume that the vectors are linearly dependent. Then, $(1,\xi,\xi^{2}) = c(1,1,1)$ for some nonzero $c\in \mathbb{C}$. Then, $1=\xi = \xi^{2}$ and so $\xi = 1$. \\

  For the converse, suppose that $\xi \neq 1$. Then, $\xi < \xi^{2}$ and so there is no nonzero scalar $c$ such that $(1,\xi,\xi^{2}) = c(1,1,1)$. 
\end{proof}
\end{solution}
  \item Under what conditions on the scalar $\xi$ do the vectors $(0,1,\xi), (\xi,0,1),$ and $(\xi,1,1+\xi)$ form a basis of $\mathbb{C}^{3}$.
\begin{solution}
  Note that for any $\xi \in \mathbb{C}$, 
\begin{align*}
  (0,1,\xi) = - (\xi,0,1) + (\xi,1,1+\xi).
\end{align*}
Hence, for every $\xi\in \mathbb{C}$, the vectors are linearly dependent and so not a basis of $\mathbb{C}^{3}$. There are no conditions for them to be a basis.
\end{solution}
\end{enumerate}
\end{problem}
\begin{problem}{10}
  If $\mathcal{X}$ is the set consisting of the six vectors $(1,1,0,0),(1,0,1,0),(1,0,0,1),(0,1,1,0),\\ (0,1,0,1),(0,0,1,1)$ in $\mathcal{C}^{4}$, find two different maximal linearly independent subsets of $\mathcal{X}$. (A maximal linearly independent subset of $\mathcal{X}$ is a linearly independent subset $\mathcal{Y}$ of $\mathcal{X}$ that becomes linearly dependent every time that a vector $\mathcal{X}$ that is not already in $\mathcal{Y}$ is adjoined to $\mathcal{Y}$). 
\begin{solution}
  Two maximal linearly independent subsets are $(0,0,1,1),(0,1,0,1),\\ (0,1,1,0),(1,0,0,1),(1,1,0,0)$ and $(1,1,0,0),(1,0,1,0),(1,0,0,1),(0,1,1,0)$. Note that we can divide the vectors in two groups, the ones with number 1 as the first entry and the ones with a 0. Furthermore, the last three entries of the vectors of each group are linearly independent and so they span the last three entries of each vector from the other group.   
\end{solution}
\end{problem}
\section{Classification of Finite Vector Spaces: dimension and isomorphism}
We know that a basis of some Vector Space $\mathcal{V}$ is a subset that is both linearly independent and a generator of $\mathcal{V}$. One can show, by repeatedly using the fact that a set is linearly independent if and only if the preceding vectors span it, that a linearly independent set has less or equal number of vectors than a generator set. Hence, any basis of the same vector space has the same number of elements. \\
This is a very useful fact, which the concept of \textit{Dimension} harnesses. One can categorize Vector Spaces into sets according to their dimension, namely, the number of elements that its basis has. Furthermore, for any $n$ finite dimensional vector space $\mathcal{U}$ over $\mathbb{F}$ with basis $\left(x_{\alpha} \right)_{\alpha\in \N}$, there is the isomorphism $\phi: \mathcal{U} \to \mathbb{F}^{n}$ such that
\begin{align*}
  x \to (\lambda^{\alpha})_{\alpha\in\N}, x=\sum\lambda_{\alpha}x_{\alpha}.
\end{align*}
since there is a one-to-one correspondence between the vector $x$ and scalars $\lambda_{\alpha}$. With this, one concludes this connects vector spaces in a way such that there is an isomorphism between two vector spaces if and only if the have the same dimension. 
\begin{problem}{1}
\begin{enumerate}
  \item What is the dimension of the set $\mathbb{C}$ of all complex numbers considered as a real vector space? 
\begin{solution}
  So the field is $\mathbb{R}$ and there is a one-to-one correspondence between each number $a+bi\in \mathbb{C}$ and $(a,b)\in \mathbb{R}^{2}$, which is also a real vector space. Hence the dimension of the real vector space $\mathbb{C}$ is 2. From this, we can prove the following useful theorem:
  \begin{theorem}{Isomorphism-dimension:}
    There is an isomorphism between two finite vector spaces over the same field if and only if they have the same dimension.
\begin{proof}
  Let $\mathcal{V},\mathcal{U}$ be vector spaces over the same field such that $\mathcal{V}\leftrightarrows \mathcal{U}$, there is an isomorphism $T$.  Both have unique basis, and so let $(x_{\lambda})_{\lambda\leq n}$ be the basis of $\mathcal{V}$. We show that $(T(x_{\lambda}))_{\lambda\leq n}$ is the basis of $\mathcal{U}$. Consider any vector $u\in \mathcal{U}$. By the surjectivity of $T$, there is some $v\in \mathcal{V}$ such that $T(v) = u$ and so, 
\begin{align*}
  T(\sum_{\lambda\leq n} \alpha_{\lambda}x_{\lambda}) = \sum_{\lambda\leq n}\alpha_{\lambda} T(x_{\lambda}). 
\end{align*}
Furthermore, if $u=0$, then $\alpha_{\lambda} = 0$ and so $(T(x_{\lambda}))$ is linearly independent and a generator of $\mathcal{U}$. It is the basis of $\mathcal{U}$. \\
To prove the converse, it suffices to note that $\mathcal{V}\leftrightarrows \mathbb{F}^{n}\leftrightarrows \mathcal{U}$, where $\mathbb{F}$ is the field of both vector spaces.
\end{proof}
\end{theorem}
\item Every complex vector space $\mathcal{V}$ is intimately associated with a real vector space $\mathcal{V}^{-}$; the space $\mathcal{V}^{-}$ is obtained from $\mathcal{V}$ by refusing to multiply vectors of $\mathcal{V}$ by anything other than real scalars. If the dimension of the complex vector space $\mathcal{V}$ is $n$, what is the dimension of the real vector space $\mathcal{V}^{-}$.
\begin{proof}
  The dimension of $\mathcal{V}^{-}$ is $2n$. We use the notation $\mathbb{R}|\mathcal{C}$ to represent the vector space $\mathcal{C}$ over $\mathbb{R}$. Note that 
\begin{align*}
  \mathbb{C}|\mathcal{V}\leftrightarrows& \mathbb{C}|\left\{ (a_{1}+b_{1}i,\dots,a_{n}+b_{n}i):a_{1},b_{1},\dots,a_{n},b_{n}\in \R \right\} \\
  \leftrightarrows& \mathbb{C}|\left\{ (a_{1},b_{1},\dots,a_{n},b_{n})|a_{1},\dots,b_{n}\in \R\right\} = \mathbb{C}|\mathbb{R}^{2n}.
\end{align*}
Hence, $\mathbb{R}|\mathcal{V}\leftrightarrows \mathbb{R}|\mathbb{R}^{2n}$ and so the dimension of $\mathcal{V}^{-}$ is $2n$.
\end{proof}
\end{solution}
\end{enumerate}
\end{problem}
\begin{problem}{2}
  Is the set $\R$ of all real numbers a finite-dimensional vector space over the field $\Q$ of all rational numbers? (The question is not trivial; it helps to know something about cardinal numbers.)
\begin{solution}
  An argument by contradiction can be given. Suppose, to the contrary, that this was true, then, clearly, $\Q|\R\leftrightarrows \Q|\Q^{n}$ for some $n$. However,
\begin{align*}
  \N \sim \Q|\Q^{n} &\leftrightarrows \Q|\R\\
  &\supseteq \left\{ 1b |  b\in \R\right\} = \R.
\end{align*}
Because the multiplication is closed in the set of real numbers, $\Q|\R = \R$. This implies that $\R$ is countable, which is not true. We arrive at a contradiction.\\
A corollary from this is that a finite vector space  over a countable field is countable.
\begin{proof}
  Let $\mathbb{D}|\mathcal{V}$ be a finite dimensional set (finite basis with $n$ elements) over some countable field $\mathbb{D}$. Then, $\mathbb{D}|\mathbb{D}^{n} = \mathbb{D}^{n} \sim \N^{n} \sim \N$. Thus, the biyection (isomorphism) between $\mathbb{D}|\mathcal{V}$ and $\mathbb{D}^{n}$ implies that the earlier is countable.
\end{proof}
\end{solution}
\end{problem}
\begin{problem}{3}
  How many vectors are there in an $n$-dimensional vector space over the field $\Z_{p}$ (where $p$ is a prime)?
\begin{solution}
  Let $\mathcal{V}$ be such vector space. Then, $\mathcal{V} \leftrightarrows \Z_{p}|\Z_{p}^{n} \leftrightarrows \Z_{p}^{n}$. Therefore, $|\Z_{p}^{n}| = p^{n} = |\mathcal{V}|$.
\end{solution}
\end{problem}
\begin{problem}{4}
  Discuss the following assertion: if two rational vector spaces have the same cardinal number (i.e., if there is some one-to-one correspondence between them), they are isomorphic (i.e., there is a linearit-preserving one-to-one correspondence between them). A knowledge of the basic facts of cardinal arithmetic is needed for an intelligent discussion.
\begin{solution}
  
\end{solution}
\end{problem}

\subsection{Subspaces: A new perspective into basis and the concept of ``span''}

Before diving into the interesting idea and usage of subsapces, we must account of some fundamental facts of vector spaces. First of all, we defined a basis $\mathcal{M}$ as a linearly independent subset of a vector space $\mathcal{V}$ such that every vector in $\mathcal{V}$ can be expressed as a linear combination of the vectors in $\mathcal{M}$. Then, using Zorn's Lemma and the fact that a set is linearly dependent if at least one vector is a linear combination of the previous ones, one can show that every vector space has at least one basis and that the cardinality is the same for all possible basis, respectively.\\

Now, a subspace $\mathcal{M}$ of a $\mathbb{F}|\mathcal{V}$ is a subset such that if $x,y\in \mathcal{M}$, then $\alpha x+\beta y\in \mathcal{M}, \alpha,\beta\in \mathbb{F}$. Clearly, this is a vector space when restricted to the operations of addition and scalar multiplication from $\mathcal{V}$ (Note that $x-x\in \mathcal{M}$). Furthermore, for any collection $\{\mathcal{M}_{p}\}$ of subsapces, their intersection $\bigcap_{p}\left\{ \mathcal{M}_{p} \right\}\supseteq \mathcal{O}=\left\{ 0 \right\}$ is a vector space.\\

This is useful, because for any linearly independent set of vectors $\mathcal{S}\subset \mathcal{V}$, we can find the intersection of all subspaces that cointain $S$, whic can be shown to be equivalent to the set of all linear combinations of the vectors in $\mathcal{S}$. Sounds familiar?. Indeed, this $\mathcal{S}$ is a basis for $\mathcal{M}$. Hence, a new way to look at a basis is the linearly independent set of vectors such that $\mathcal{M}$ is the set of all linear combinations of these vectors. Then, it is said that the linearly independent set ``spans'' $\mathcal{M}$. Also, the dimension of $\mathcal{M}$ is the cardinality of this linearly independent set. 
\begin{problem}{1}
  If $\mathcal{M}$ and $\mathcal{N}$ are finite-dimensional subspaces with the same dimension, and if $\mathcal{M}\subset \mathcal{N}$, then $\mathcal{M}=\mathcal{N}$.
\begin{proof}
 Since $\mathcal{M}\subset \mathcal{N}$, it follows that $\mathcal{M}$ is a subspace of the vector space $\mathcal{N}$. Hence, by theorem 2, we can find a basis $K$ in $\mathcal{N}$ so that $L\subseteq K$ forms a basis in $\mathcal{M}$. However, both vector spaces have the same dimension and so $|K|=|L|=n$, which implies that $K=L$. Thus, $\mathcal{M}=\mathcal{N}$.
\end{proof}
\end{problem}

\begin{problem}{2}
  If $\mathfrak{M}$ and $\mathfrak{N}$ are subspaces of a vector space $\mathfrak{V}$, and if every vector in $\mathfrak{V}$ belongs either to $\mathfrak{M}$ or to $\mathfrak{N}$ (or both), then either $\mathfrak{M} = \mathfrak{V}$ or $\mathfrak{N} = \mathfrak{V}$ (or both).
\begin{proof}
  Suppose, without loss of generality, that every vector in $\mathfrak{V}$ belongs to $\mathfrak{M}$. Then, $\mathfrak{V}\subseteq \mathfrak{M}$ and $\mathfrak{M}\subseteq \mathfrak{V}$, since $\mathfrak{M}$ is a subspace of $\mathfrak{V}$. Hence, $\mathfrak{M}=\mathfrak{V}$. 
\end{proof}
\end{problem}
\begin{problem}{3}
  If $x,y,$ and $z$ are vectors such that $x+y+z=0$, then $x$ and $y$ span the same subspace as $y$ and $z$.
\begin{proof}
  Consider the subspaces $\mathfrak{Z} = \left\{ \gamma z : \gamma \in \mathbb{F} \right\}, \mathfrak{X} = \left\{ \alpha x : \alpha\in \mathbb{F}\right\}$ and $\mathfrak{Y} = \left\{ \beta y : \beta\in \mathbb{F}\right\}$, where $\mathbb{F}$ is the field. Then, 
\begin{align*}
  \mathfrak{X}+\mathfrak{Y} &= \left\{ \alpha x + \beta y : \alpha,\beta\in \mathbb{F}\right\}\\
  &= \left\{ \alpha(z-y)+\beta y : \alpha,\beta\in \mathbb{F} \right\} \\
  &= \left\{ \alpha z + \gamma y: \alpha,\gamma\in \mathbb{F} \right\}\\
  &= \mathfrak{Y}+\mathfrak{Z}
\end{align*}
because $\mathbb{F}$ is closed under addition. 
\end{proof}
From this, we can easily derive a corollary:\\

If $x_{1}+x_{2}+\dots+x_{n} = 0$, then any subspace spanned by a pair of the vectors $x_{k}$ spans the space spanned by the rest plus at least one of the pair. The need of a pair must have to do with the binary nature of addition. 
\end{problem}
\begin{problem}{4}
  Suppose that $x$ and $y$ are vectors and $\mathfrak{M}$ is a subspace in a vector space $\mathfrak{V}$; let $\mathfrak{H}$ be the subspace spanned by $\mathfrak{M}$ and $x$, and let $\mathfrak{K}$ be the subspace spanned by $\mathfrak{M}$ and $y$. Prove that if $y$ is in $\mathfrak{H}$ but not in $\mathfrak{M}$, then $x$ is in $\mathfrak{K}$. 
\begin{proof}
  We know that $\left\{ \alpha x \right\} + \mathfrak{M} = \mathfrak{H}$ and $\left\{ \beta y \right\} + \mathfrak{M} = \mathfrak{K}$. Since $y\in \mathfrak{H}$, it follows that $y= \alpha x+ m$ for some $m\in \mathfrak{M}$. Furtheremore, $\alpha \neq 0$ because $y\not\in \mathfrak{M}$. Thus, $x = \alpha^{-1} y - \alpha^{-1} m \in \mathfrak{K}$.
\end{proof}
\end{problem}
\begin{problem}{5}
  Suppose that $\mathfrak{L},\mathfrak{M},$ and $\mathfrak{N}$ are subspaces of a vector space.
\begin{enumerate}
  \item Show that the equation 
\begin{align*}
  \mathfrak{L}\cap (\mathfrak{M}+\mathfrak{N}) = (\mathfrak{L}\cap\mathfrak{M})+ (\mathfrak{L}\cap\mathfrak{N})
\end{align*}
is not necessarily true.
\begin{proof}
  Let $\mathfrak{M} = \left\{ \alpha m \right\}, \mathfrak{N} = \left\{ \alpha n \right\}$ and $\mathfrak{L} = \left\{ \alpha(n+m) \right\}$, where $\left\{ m,n \right\}$ are linearly independent. Then, 
\begin{align*}
  \mathfrak{L}\cap(\mathfrak{M}+\mathfrak{N}) &= \mathfrak{L}\\
  &\neq \left\{ 0 \right\} = \mathfrak{L}\cap \mathfrak{M} + \mathfrak{L}\cap \mathfrak{N}.
\end{align*}
The subspaces $\mathfrak{L},\mathfrak{M},\mathfrak{N}$ can be seen as lines in the plane $\mathfrak{M}+\mathfrak{N}$ that only intersect at the origin.
\end{proof}
\item Prove that 
\begin{align*}
  \mathfrak{L}\cap(\mathfrak{M}+(\mathfrak{L}\cap\mathfrak{N})) = (\mathfrak{L}\cap \mathfrak{M}) + (\mathfrak{L}\cap \mathfrak{N})
\end{align*}
\begin{proof}
  Let $\mathfrak{B} = \mathfrak{L}\cap \mathfrak{N}$. Since $0\in \mathfrak{M},\mathfrak{B}$, it follows that $\left\{ m +0 \right\} = \mathfrak{M}$ and $\left\{b + 0 \right\} = \mathfrak{B}$ are subsets of $\mathfrak{M}+\mathfrak{B}$ and so
\begin{align*}
  \mathfrak{B},  \mathfrak{L}\cap\mathfrak{M} &\subseteq \mathfrak{L}\cap(\mathfrak{M}+\mathfrak{B})\implies\\
  \mathfrak{L}\cap\mathfrak{M} + \mathfrak{B} &\subseteq \mathfrak{L}\cap(\mathfrak{M}+\mathfrak{B}).
\end{align*}
Now, consider any $x\in \mathfrak{L}\cap \left( \mathfrak{M}+\left( \mathfrak{L}\cap\mathfrak{N} \right) \right)$. Then, $x= m+n$, where $m\in \mathfrak{M}$ and $n\in \mathfrak{N}\cap\mathfrak{L}$. Then, $-n \in \mathfrak{L}\cap (\mathfrak{M} + \mathfrak{N}\cap\mathfrak{L})$ and so $m+n-n = m \in \mathfrak{L}\cap\left( \mathfrak{M}+\mathfrak{N}\cap\mathfrak{L} \right)$. Thus, $m\in \mathfrak{L}\cap\mathfrak{M}$ and so $m+n=x\in \mathfrak{L}\cap\mathfrak{M} + \mathfrak{L}\cap\mathfrak{N}$.\\

Something interesting about subspaces, resembling vectors, is that
\begin{align*}
  \mathfrak{A},\mathfrak{B}&\subseteq V\implies\\
  \mathfrak{A}+\mathfrak{B}&\subseteq V.
\end{align*}
Just change $\subseteq$ for $\in$ and let $\mathfrak{A},\mathfrak{B}$ be vectors to note the resemblance.
\end{proof}
\end{enumerate}
\end{problem}
\begin{problem}{6}
\begin{enumerate}
  \item Can it happen that a non-trivial subspace of a vector space $\mathcal{V}$ (i.e., a subspace different from both $\mc{O}$ and $\mc{V}$) has a unique complement?
\begin{proof}
  Let $\mathcal{V}$ be a vector space of dimension $\geq 2$ and $\mf{X}$ be a non-trivial subspace with complement $\mf{Y}$. Then, $\mf{X}\cap\mf{Y} = \mc{O}$ and $\mf{X} + \mf{Y} = \mc{V}$. Thus, the basis of $\mf{X}$ and $\mf{Y}$ are complementary. Now, consider some basis vector $x_{1}$ of $\mf{X}$ and other $y_{1}$ of $\mf{Y}$. We show that the subspace $\mf{B} = \left\{ \alpha_{1}(x_{1}+y_{1}) + \alpha_{2} y_{2} + \alpha_{3} y_{3} + \dots\right\}$ is a different complement of $\mf{X}$.\\
  Because the union of the basis of $\mf{X}$ and $\mf{Y}$ is a linearly independent basis that spans $\mc{V}$, it follows that $\left\{ x_{1}+y_{1},y_{2},y_{3},\dots \right\}$ is linearly independent and $x_{1}+y_{1}\not\in \mf{Y},\mf{X}$. Furthermore,
\begin{align*}
  \alpha_{1}(x_{1}+y_{1})+\alpha_{2} y_{2} + \dots &= \beta_{1} x_{1} + \beta_{2} x_{2}+ \dots \implies\\
  (\alpha_{1}-\beta_{1})x_{1} + \alpha_{1}y_{1} - \beta_{2}x_{2} + \alpha_{2}y_{2} + \dots &= 0\implies\\
  \alpha_{1}=\beta_{1} =\alpha_{2} = \beta_{2} = \dots &= 0.
\end{align*}

  Hence, $\mf{X}\cap\mf{B} = \mc{O}$, $\mf{B}\neq \mf{Y}$ and $\mf{B}+\mf{X} = \mc{V}$.
\end{proof}
  \item If $\mc{M}$ is an $m$-dimensional subspace in an $n$-dimensional vectors space, then every complement of $\mc{M}$ has dimension $n-m$.
\begin{proof}
  Let $\mc{X}$ be the complement of $\mc{M}$ with dimension $x$. Then, the dimension  $\dim(\mc{M}+\mc{X}) = n$. Furthermore, let $B = \left\{ b_{1},b_{2},\dots,b_{m} \right\}$ and $A=\left\{ a_{1},a_{2},\dots,a_{x} \right\}$ be basis of $\mc{M}$ and $\mc{X}$, respectively. We show that $A\cup B$ is a basis of $\mc{M}+\mc{X}$. First, since, by definition, $\mc{M}\cap \mc{X}=\left\{ 0 \right\}$, it follows that $A\cup B$ is linearly independent (Note that if the contrary is true, then it implies that some vector of a set can be expressed as a linear combination of the basis of the other set and so $\mc{M}\cap\mc{X} \supset \left\{ 0 \right\}$).\\
  Now, consider any vector $a\in \mc{M}+\mc{X}$. Then, $a=x+m$, for some $x\in \mc{X}$ and $m\in \mc{M}$. Thus,  
\begin{align*}
  x+m &= (\sum_{i=1}^{x}\beta_{i}a_{i})+(\sum_{k=1}^{m}\alpha_{k}b_{k}).
\end{align*}
Hence, $|A\cup B| = m+x=n$ and so $x=n-m$.
\end{proof}
\end{enumerate}
\end{problem}
\begin{problem}{7}
\begin{enumerate}
  \item Show that if both $\mathcal{M}$ and $\mathcal{N}$ are three-dimensional subspaces of a five-dimensional vector space, then $\mathcal{M}$ and $\mathcal{N}$ are not disjoint.
\begin{proof}
  Let $A$ and $B$ be bases of $\mathcal{M}$ and $\mathcal{N}$, respectively. Then, $A\cup B$ is linearly dependent since they are vectors of a 5-dimensional vector space (the maximal cardinality of a lineraly independent set is $5$). Therefore, there is a vector from a vector subspace that can be expressed as a linear combination of the others, where at least one vector of the other subspace has a nonzero scalar. Hence, it can easily be shown that a linear combination of vectors of $\mathcal{M}$ and be expressed as a linear combination in $\mathcal{N}$. Thus, $\mathcal{M}\cap\mathcal{N} \supset \left\{ 0 \right\}$.  
\end{proof}
  \item If $\mathcal{M}$ and $\mathcal{N}$ are finite-dimensional subspaces of a vector space, then
\begin{align*}
  \text{dim}\mathcal{M} + \text{dim}\mathcal{N} = \text{dim}(\mathcal{M}+\mathcal{N}) + \text{dim}(\mathcal{M}\cap\mathcal{N}).
\end{align*}
\begin{proof}

\end{proof}
\end{enumerate}
\end{problem}
\end{document}

